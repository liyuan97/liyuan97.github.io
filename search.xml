<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>通过ssh免密部署github</title>
    <url>/2020/github-ssh/</url>
    <content><![CDATA[<p>之前在部署博客到github page的时候每当遇到hexo deploy，都遇到了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git@github.com: Permission denied (publickey).</span><br><span class="line">fatal: 无法读取远程仓库。</span><br><span class="line">请确认您有正确的访问权限并且仓库存在。</span><br></pre></td></tr></table></figure><br>搞的我百思不得其解，之前ssh和github的连接明明是搞好的。最后定位到问题是系统ssh-key代理被误删了。具体是什么操作误删我就没定位到，可能是因为用了zsh的shell之后，和之前bash的路径不对？算了，这篇就总结一下ssh的过程。</p><a id="more"></a>
<p>具体怎么搞ssh参考这篇<a href="https://blog.csdn.net/u013778905/article/details/83501204" target="_blank" rel="noopener">GitHub如何配置SSH Key</a>。</p>
<h1 id="设置git的user-name和email"><a href="#设置git的user-name和email" class="headerlink" title="设置git的user name和email"></a>设置git的user name和email</h1><p>如果你是第一次使用，或者还没有配置过的话需要操作一下命令，自行替换相应字段。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;xxxxxx&quot;</span><br><span class="line">git config --global user.email  &quot;xxxxx@gmail.com&quot;</span><br></pre></td></tr></table></figure><br>说明：git config —list 查看当前Git环境所有配置，还可以配置一些命令别名之类的</p>
<h1 id="检查-生成ssh-key"><a href="#检查-生成ssh-key" class="headerlink" title="检查/生成ssh key"></a>检查/生成ssh key</h1><p>ssh key 由一个公钥(id_rsa.pub)和私钥(id_rsa)组成，构成ssh通道的安全。<br>可以通过cd ~/.ssh   然后ls  查看。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226175214.png" alt><br>上图中，我有两组ssh key。<br>如果没有，就生成一下。</p>
<blockquote>
<p>ssh-keygen -t rsa -C “xxxxxxx@qq.com”</p>
</blockquote>
<h1 id="获取公钥添加到github里"><a href="#获取公钥添加到github里" class="headerlink" title="获取公钥添加到github里"></a>获取公钥添加到github里</h1><blockquote>
<p>cat id_rsa.pub<br>然后复制到github -&gt; settting -&gt; ssh and GPG keys -&gt; new SSHkey</p>
</blockquote>
<p>然后拷贝进去。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226175610.png" alt></p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><div class="note primary">
            <blockquote><p>$ ssh -T git@github.com<br>Hi ! You’ve successfully authenticated, but GitHub does not provide shell access.</p></blockquote>
          </div>
<p>说明运行成功，恭喜你拜托了https步入SSH时代。</p>
<h1 id="解决多个秘钥ssh权限问题"><a href="#解决多个秘钥ssh权限问题" class="headerlink" title="解决多个秘钥ssh权限问题"></a>解决多个秘钥ssh权限问题</h1><p>通常一台电脑生成一个ssh不会有这个问题，当一台电脑生成多个ssh的时候，就可能遇到这个问题，解决步骤如下：</p>
<h2 id="查看系统ssh-key代理-执行如下命令"><a href="#查看系统ssh-key代理-执行如下命令" class="headerlink" title="查看系统ssh-key代理,执行如下命令"></a>查看系统ssh-key代理,执行如下命令</h2><p><code>$ ssh-add -l</code></p>
<p>　　以上命令如果输出 The agent has no identities. 则表示没有代理。如果系统有代理，可以执行下面的命令清除代理:<br><code>$ ssh-add -D</code></p>
<h2 id="然后依次将不同的ssh添加代理，执行命令如下："><a href="#然后依次将不同的ssh添加代理，执行命令如下：" class="headerlink" title="然后依次将不同的ssh添加代理，执行命令如下："></a>然后依次将不同的ssh添加代理，执行命令如下：</h2><p><code>$ ssh-add ~/.ssh/id_rsa</code><br><code>$ ssh-add ~/.ssh/aysee</code></p>
<p>　你会分别得到如下提示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2048 8e:71:ad:88:78:80:b2:d9:e1:2d:1d:e4:be:6b:db:8e /Users/aysee/.ssh/id_rsa (RSA)</span><br><span class="line">2048 8e:71:ad:88:78:80:b2:d9:e1:2d:1d:e4:be:6b:db:8e /Users/aysee/.ssh/id_rsa (RSA)</span><br></pre></td></tr></table></figure><br>　　如果使用 ssh-add ~/.ssh/id_rsa的时候报如下错误，则需要先运行一下 ssh-agent bash 命令后再执行 ssh-add …等命令<br><code>Could not open a connection to your authentication agent.</code></p>
<h2 id="配置-ssh-config-文件"><a href="#配置-ssh-config-文件" class="headerlink" title="配置 ~/.ssh/config 文件"></a>配置 ~/.ssh/config 文件</h2><p>　　如果没有就在~/.ssh目录创建config文件，该文件用于配置私钥对应的服务器<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Default github user(first@mail.com)</span><br><span class="line">Host github.com</span><br><span class="line">HostName github.com</span><br><span class="line">User git</span><br><span class="line">IdentityFile C:/Users/username/.ssh/id_rsa</span><br><span class="line"># aysee (company_email@mail.com)</span><br><span class="line">Host github-aysee</span><br><span class="line">`HostName github.com</span><br><span class="line">User git</span><br><span class="line">IdentityFile C:/Users/username/.ssh/aysee</span><br></pre></td></tr></table></figure><br>Host随意即可，方便自己记忆，后续在添加remote是还需要用到。 配置完成后，在连接非默认帐号的github仓库时，远程库的地址要对应地做一些修改，比如现在添加second帐号下的一个仓库test，则需要这样添加：</p>
<p><code>git remote add test git@github-aysee:ay-seeing/test.git</code><br><code>#并非原来的git@github.com:ay-seeing/test.git</code><br>ay-seeing 是github的用户名</p>
<h2 id="测试-ssh"><a href="#测试-ssh" class="headerlink" title="测试 ssh"></a>测试 ssh</h2><p><code>ssh -T git@github.com</code><br>你会得到如下提示，表示这个ssh公钥已经获得了权限<br><code>Hi USERNAME! You&#39;ve successfully authenticated, but github does not provide shell access.</code></p>
<p>参考：<a href="https://www.cnblogs.com/ayseeing/p/4445194.html" target="_blank" rel="noopener">https://www.cnblogs.com/ayseeing/p/4445194.html</a></p>
]]></content>
      <tags>
        <tag>ssh</tag>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>MSBD5009-1 并行计算介绍</title>
    <url>/2020/MSBD5009-1-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h1 id="并行计算parallel-programming"><a href="#并行计算parallel-programming" class="headerlink" title="并行计算parallel programming"></a>并行计算parallel programming</h1><p>选这门课主要是想提高编程能力，顺便了解一下os相关知识，本科没学过操作系统和编译原理什么的，经常遇到了云里雾里的。<br>还有以前还去碰过CUDA，自己瞎研究深度学习的项目，最后连CUDA和TF的版本适配都没解决，害，一把辛酸泪。<br>第一节课遇到各种线程进程cache，中文都不知道是啥，听得云里雾里的。</p><a id="more"></a>
<hr>
<p>这门课的分数构成：<br>50分的三次编程作业，每次作业是对已经有的代码填空。<br>50分的考试，有编程填题，也有简答题。</p>
<hr>
<p>三本参考书：</p>
<ul>
<li>Introduction to Parallel Computing 2nd edition</li>
<li>An Introduction to Parallel Programming</li>
<li>Programming Massively Parallel Processors:<br>A Hands-on Approach 3rdd Edition</li>
</ul>
<blockquote>
<p>不出意外是不会看了🤣，以后需要再说吧。</p>
</blockquote>
<h1 id="并行计算的基本背景"><a href="#并行计算的基本背景" class="headerlink" title="并行计算的基本背景"></a>并行计算的基本背景</h1><h2 id="为什么我们需要一度的提高性能-表现？"><a href="#为什么我们需要一度的提高性能-表现？" class="headerlink" title="为什么我们需要一度的提高性能/表现？"></a>为什么我们需要一度的提高性能/表现？</h2><p>随着数据的增长和计算的需求(基因计算，深度学习，天文计算，天气模型，蛋白质计算等等)增加，cpu的性能越来越不够用，简单的办法就是增加核心数。<br>核心数增加了就需要让程序能充分利用起来各个核心，就像老板跟员工派任务一样，需要充分榨干劳动力。</p>
<h2 id="为什么需要构建并行系统"><a href="#为什么需要构建并行系统" class="headerlink" title="为什么需要构建并行系统"></a>为什么需要构建并行系统</h2><p>芯片的瓶颈难以突破，只有从系统上高效调用。<br>瓶颈：高密度的晶状体-&gt; 更快的芯片-&gt;增加耗能-&gt;增加发热-&gt;导致芯片不稳定<br>所以需要多核合作。</p>
<h2 id="为什么需要写并行的程序"><a href="#为什么需要写并行的程序" class="headerlink" title="为什么需要写并行的程序"></a>为什么需要写并行的程序</h2><p>直接改写串行的程序可能会遇到各种复杂的问题，数据结构，算法设计方面的。改写出来的可能依然不高效。</p>
<h2 id="怎么写并行程序"><a href="#怎么写并行程序" class="headerlink" title="怎么写并行程序"></a>怎么写并行程序</h2><ul>
<li>任务划分<br>例子：改卷每人改一道题</li>
<li>数据划分<br>例子：改卷每人改100张卷</li>
</ul>
<p>需要解决多核合作的问题：</p>
<ul>
<li><p>Communication – one or more cores send their current partial sums to another core. 相互沟通</p>
</li>
<li><p>Load balancing – share the work evenly among the cores so that one is not heavily loaded. 任务量均衡</p>
</li>
<li><p>Synchronization – because each core works at its own pace, make sure cores do not get too far ahead of the rest. 齐头并进</p>
</li>
</ul>
<h2 id="concurrent-parallel-distributed"><a href="#concurrent-parallel-distributed" class="headerlink" title="concurrent,parallel,distributed"></a>concurrent,parallel,distributed</h2><p>• Concurrent computing – In a program multiple tasks can be in progress at any instant. 程序同时处理多个任务    </p>
<p>• Parallel computing – In a program multiple tasks cooperate closely to solve a problem.<br>多个任务协作</p>
<p>• Distributed computing – A program may need to cooperate with other programs to solve a problem.<br>程序之间协作</p>
<blockquote>
<p>没搞懂这里的tasks和program之间的区别</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>并行系统是计算趋势</li>
<li>串行的程序不能用上多核</li>
<li>要学习如何要调用内核</li>
<li>也需要可靠的开发技术</li>
</ul>
<h1 id="硬件和操作系统"><a href="#硬件和操作系统" class="headerlink" title="硬件和操作系统"></a>硬件和操作系统</h1><h2 id="冯·诺伊曼模型"><a href="#冯·诺伊曼模型" class="headerlink" title="冯·诺伊曼模型"></a>冯·诺伊曼模型</h2><p>[百度百科]<br>冯·诺依曼型计算机一般具有以下五个功能：必须具有长期记忆程序、数据、中间结果及最终运算结果的能力；能够完成各种算术、逻辑运算和数据传送等数据加工处理的能力；能够根据需要控制程序走向，并能根据指令控制机器的各部件协调操作；能够按照要求将处理结果输出给用户。</p>
<p>冯·诺依曼型计算机从本质上讲是采取串行顺序处理的工作机制，即使有关数据巳经准备好，也必须逐条执行指令序列。而提高计算机性能的根本方向之一是并行处理。因此，近年来人们谋求突破传统冯·诺依曼体制的束缚，这种努力被称为非诺依曼化。对所谓非诺依曼化的探讨仍在争议中，一般认为它表现在以下三个方面的努力。<br>　　（1）在冯·诺依曼体制范畴内，对传统冯·诺依曼机进行改造，如采用多个处理部件形成流水处理，依靠时间上的重叠提高处理效率；又如组成阵列机结构，形成单指令流多数据流，提高处理速度。这些方向已比较成熟，成为标准结构；<br>　　（2）用多个冯·诺依曼机组成多机系统，支持并行算法结构。这方面的研究目前比较活跃；<br>　　（3）从根本上改变冯·诺依曼机的控制流驱动方式。例如，采用数据流驱动工作方式的数据流计算机，只要数据已经准备好，有关的指令就可并行地执行。这是真正非诺依曼化的计算机，它为并行处理开辟了新的前景，但由于控制的复杂性，仍处于实验探索之中。</p>
<h2 id="Central-processing-unit-CPU"><a href="#Central-processing-unit-CPU" class="headerlink" title="Central processing unit (CPU)"></a>Central processing unit (CPU)</h2><p>• Control unit - responsible for deciding which instruction in a program should be executed. (the boss) 控制单元</p>
<p>• Arithmetic and logic unit (ALU) - responsible for executing the actual instructions. (the worker) 计算单元</p>
<h2 id="关键terms"><a href="#关键terms" class="headerlink" title="关键terms"></a>关键terms</h2><p>• Register – very fast storage, part of the CPU.</p>
<p>• Program counter – stores address of the next instruction to be executed.</p>
<p>• Bus – wires that connect the CPU and memory.</p>
<p>冯·诺伊曼模型的瓶颈在于CPU和内存的分离，cpu的寄存器太小，必须和内存交换数据，但是互联的速率有限。</p>
<h2 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h2><p>• The same or nearby locations are accessed frequently.<br>• Spatial locality – accessing a nearby location.<br>• Temporal locality – accessing in the near future.</p>
<h2 id="从cpu到cache的问题"><a href="#从cpu到cache的问题" class="headerlink" title="从cpu到cache的问题"></a>从cpu到cache的问题</h2><p>由于cache分三级，很有可能写入不连续。<br>–直写式  通过在写入高速缓存时更新主内存中的数据来处理此问题。<br>–回写式 将缓存cache中的数据标记为脏数据。当高速缓存行由内存中的新高速缓存行替换时，脏行将写入内存。</p>
<h2 id="缓存映射Cache-Mapping"><a href="#缓存映射Cache-Mapping" class="headerlink" title="缓存映射Cache Mapping"></a>缓存映射Cache Mapping</h2><p>• 完全关联–可以在缓存中的任何位置放置新行。<br>• 直接映射–每条缓存行在缓存中都有一个唯一的位置，将为其分配该位置。<br>• n向集合关联–每个高速缓存行可以放置在高速缓存中n个不同位置之一中。</p>
<h2 id="缓存逐出Cache-Eviction"><a href="#缓存逐出Cache-Eviction" class="headerlink" title="缓存逐出Cache Eviction"></a>缓存逐出Cache Eviction</h2><p>•缓存比主内存小得多。<br>•当缓存已满时，需要在内存中添加新行以替换或逐出缓存中的行。<br>•常见的缓存逐出策略包括LRU / MRU（最近最少使用/最近使用）和LFU（最近最少使用）。</p>
<h2 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h2><p>•如果我们运行非常大的程序或访问<strong>非常大</strong>的数据集的程序，则所有指令和数据可能无法放入主存储器。<br>•虚拟内存用作<strong>辅助存储的缓存</strong>。<br>•它利用<strong>时空局部性</strong>原理。<br>•它仅将正在运行的程序的活动部分保留在主存储器中。<br>•<strong>交换空间Swap space</strong>–辅助存储区，用于保持非活动（部分）正在运行的程序。<br>•<strong>虚拟的页page</strong>–数据和指令块。大多数系统具有固定的页面大小，当前范围为4到16 KB。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226170253.png" alt></p>
<h2 id="虚拟页码"><a href="#虚拟页码" class="headerlink" title="虚拟页码"></a>虚拟页码</h2><p>•编译程序时，其页面为分配的虚拟页码。<br>•运行程序时，将创建一个表，该表将虚拟页码映射到物理地址。<br>•页表用于将虚拟地址转换为物理地址。</p>
<h2 id="转换后备缓冲器（Translation-lookaside-buffer-TLB）"><a href="#转换后备缓冲器（Translation-lookaside-buffer-TLB）" class="headerlink" title="转换后备缓冲器（Translation-lookaside buffer TLB）"></a>转换后备缓冲器（Translation-lookaside buffer TLB）</h2><p>•使用页表有可能显着增加每个程序的整体运行时间。<br>•TLB是处理器中的特殊地址转换缓存。<br>•它在非常<strong>快的内存中从页表中缓存了少量条目</strong>（通常为16–512）。<br>•页面错误–尝试访问页面表中页面的有效物理地址，但该页面仅存储在磁盘上。</p>
<h2 id="指令级并行（Instruction-Level-Parallelism-ILP）"><a href="#指令级并行（Instruction-Level-Parallelism-ILP）" class="headerlink" title="指令级并行（Instruction Level Parallelism ILP）"></a>指令级并行（Instruction Level Parallelism ILP）</h2><p>•试图通过使多个处理器组件或功能单元同时执行指令来提高处理器性能。<br>•流水线Pipelining-功能单元是分阶段安排的。<br>•多个问题-可以同时启动多个指令。</p>
<h2 id="硬件多线程Hardware-multithreading"><a href="#硬件多线程Hardware-multithreading" class="headerlink" title="硬件多线程Hardware multithreading"></a>硬件多线程Hardware multithreading</h2><p>•并非总是有机会同时执行不同的线程。<br>•硬件多线程为系统提供了一种在当前正在执行的任务停滞后继续进行有用工作的方法。<br>– 例如，当前任务必须等待从内存加载数据。<br>•细粒度-处理器在每条指令后在线程之间切换，跳过停滞的线程。<br>–优点：可以避免因停转而浪费机器时间。<br>–缺点：准备好执行长指令序列的线程可能必须等待执行每条指令。<br>•粗粒度-仅切换等待耗时的操作完成而停止的线程。<br>–优点：切换线程几乎不需要即时。<br>–缺点：处理器可以在较短的停顿时间内空闲，并且线程切换也将导致延迟。<br>•同步多线程（SMT）-细粒度多线程的变体。<br>•允许多个线程使用多个功能单元。</p>
]]></content>
      <categories>
        <category>MSBD5009</category>
      </categories>
      <tags>
        <tag>并行计算</tag>
      </tags>
  </entry>
  <entry>
    <title>CSIT5500-1 算法基础</title>
    <url>/2020/CSIT5500-1/</url>
    <content><![CDATA[<p>这是学校csit5500的一门课，听着名字就知道很经典。第一节课主要是算法评估，排序，搜索树，红黑树</p><h1 id="算法评估"><a href="#算法评估" class="headerlink" title="算法评估"></a>算法评估</h1><p>这一节就解释，为什么用O(n)来表示算法的复杂度。</p><ul>
<li>不能简单用cpu的时间，因为不固定</li>
<li>不同级数之间差异大，同一个级数内差异不大</li>
</ul><a id="more"></a>


<p>其他的点：</p>
<ul>
<li>比复杂度的时候，只看最高阶；</li>
<li>对数的级数都是一样的；loga n = logb n/(logb a)</li>
</ul>
<p>算法需要最差评估复杂度，一般不用证明，举例就行。</p>
<p>eg：Worst-Case Analysis of Binary Search<br>二分搜索最差的情况就是一直排除到最后一半只有一个元素的时候。<br>复杂度如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} T(n) & \leq T(n / 2)+O(1) \\ & \leq T(n / 4)+O(1)+O(1) \\ & \leq T\left(n / 2^{k}\right)+k \cdot O(1) \end{aligned}</script><p>k可以人为指定成log2(n)</p>
<script type="math/tex; mode=display">
\begin{aligned} T(n) & \leq T(1)+\log _{2} n \cdot O(1) \\ &=O(1)+O(\log n) \\ &=O(\log n) \end{aligned}</script><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><p>又到了经典的排序环节，直接引用我觉得写得最完整的两个博文<a href="https://www.cnblogs.com/onepixel/articles/7674659.html" target="_blank" rel="noopener">十大排序动图演示</a><br><a href="https://leetcode.com/problems/sort-an-array/discuss/276916/Python-bubble-insertion-selection-quick-merge-heap-objects" target="_blank" rel="noopener">python十大排序</a> </p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200225022848.png" alt></p>
<p>课上跳过了最简单的主要讲了O(nlogn)的三个，并归，快速，堆。</p>
<h2 id="并归merge"><a href="#并归merge" class="headerlink" title="并归merge"></a>并归merge</h2><p>简单的dc思路，先对数组分成两半，左右各自排序，然后用两个指针同时扫一遍，合起来。<br>这个过程递归到，两半只有一个元素的时候，就可以了。</p>
<ul>
<li>最坏情况复杂度<br>并归有两个过程，第一，遍历数组，统计长度，复杂度O(N).<br>第二，merge.<br>最好情况是，左：12345 右：6789，这样复杂度是O(N/2)=O(N).<br>最差情况是，左：13579 右：2468，这样复杂度也是O(N).<br>所以，两个过程加起来也是O(N)。</li>
</ul>
<p>然后进入到递归的过程：与二分搜索相似</p>
<script type="math/tex; mode=display">
\begin{aligned} T(n) & \leq 2 T(n / 2)+O(n) \\ & \leq 2(2 T(n / 4)+O(n / 2))+O(n) \\ & \leq 4 T(n / 4)+O(n)+O(n) \\ & \leq 2^{k} T\left(n / 2^{k}\right)+k \cdot O(n) \end{aligned}</script><p>直接取k=log2n</p>
<script type="math/tex; mode=display">
\begin{aligned} T(n) & \leq 2^{\log _{2} n} T(1)+\log _{2} n \cdot O(n) \\ & \leq O(n)+O(n \log n) \\ & \leq O(n \log n) \end{aligned}</script><p>所以最坏情况也是这个，并归排序复杂度是稳定的。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @mergeSort</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">mergeSort</span><span class="params">(self, nums)</span>:</span> </span><br><span class="line">       <span class="keyword">if</span> len(nums) &gt; <span class="number">1</span>: </span><br><span class="line">           mid = len(nums)//<span class="number">2</span></span><br><span class="line">           L = nums[:mid] </span><br><span class="line">           R = nums[mid:] </span><br><span class="line"></span><br><span class="line">           self.mergeSort(L)</span><br><span class="line">           self.mergeSort(R)</span><br><span class="line"></span><br><span class="line">           i = j = k = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">           <span class="keyword">while</span> i &lt; len(L) <span class="keyword">and</span> j &lt; len(R): </span><br><span class="line">               <span class="keyword">if</span> L[i] &lt; R[j]: </span><br><span class="line">                   nums[k] = L[i] </span><br><span class="line">                   i+=<span class="number">1</span></span><br><span class="line">               <span class="keyword">else</span>: </span><br><span class="line">                   nums[k] = R[j] </span><br><span class="line">                   j+=<span class="number">1</span></span><br><span class="line">               k+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">           <span class="keyword">while</span> i &lt; len(L): </span><br><span class="line">               nums[k] = L[i] </span><br><span class="line">               i+=<span class="number">1</span></span><br><span class="line">               k+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">           <span class="keyword">while</span> j &lt; len(R): </span><br><span class="line">               nums[k] = R[j] </span><br><span class="line">               j+=<span class="number">1</span></span><br><span class="line">               k+=<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="快速"><a href="#快速" class="headerlink" title="快速"></a>快速</h2><p>快排的思路是任选一个元素，然后把数组分成两半，再分别对两半进行这个过程。也是dc的思路。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">	<span class="comment"># @quickSort</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quickSort</span><span class="params">(array)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> len(array) &lt; <span class="number">2</span>:  <span class="comment"># 基线条件（停止递归的条件）</span></span><br><span class="line">		<span class="keyword">return</span> array</span><br><span class="line">	<span class="keyword">else</span>:  <span class="comment"># 递归条件</span></span><br><span class="line">		baseValue = array[<span class="number">0</span>]  <span class="comment"># 选择基准# 由所有小于基准值的元素组成的子数组</span></span><br><span class="line">		less = [m <span class="keyword">for</span> m <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> m &lt; baseValue]<span class="comment"># 包括基准在内的同时和基准相等的元素，在上一个版本的百科当中，并没有考虑相等元素</span></span><br><span class="line">		equal = [w <span class="keyword">for</span> w <span class="keyword">in</span> array <span class="keyword">if</span> w ==baseValue]<span class="comment"># 由所有大于基准值的元素组成的子数组</span></span><br><span class="line">		greater = [n <span class="keyword">for</span> n <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> n &gt; baseValue]</span><br><span class="line">	<span class="keyword">return</span> quickSort(less) + equal + quickSort(greater)</span><br></pre></td></tr></table></figure>
<h1 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h1><p>堆排序就高端一点，用了根堆(heap)的结构。<br>巧妙的是，通过利用完全二叉树的结构，通过数组的位置，就实现了这个根堆的过程。</p>
<h2 id="复习树🌲的知识："><a href="#复习树🌲的知识：" class="headerlink" title="复习树🌲的知识："></a>复习树🌲的知识：</h2><ul>
<li>二叉树：每一个节点最多两个孩子</li>
<li>满二叉树Full binary tree：每个子节点都有两个元素</li>
<li>完美二叉树Perfect binary tree：每一层都填满了</li>
<li>完全二叉树Complete binary tree：除了最后一层其他每一层都是完全填满的，而最后一层从左往右依次排列</li>
</ul>
<h2 id="堆排序的过程"><a href="#堆排序的过程" class="headerlink" title="堆排序的过程"></a>堆排序的过程</h2><ul>
<li>def: 小(大)根堆：完全二叉树 + 每个节点小于(大于)他的子节点。<blockquote>
<p>先把数组构造成最小根堆(最大或者最小都行)<br>while 根堆还有元素:<br>——-弹出顶部最大值<br>——-重新构造最小根堆</p>
</blockquote>
</li>
</ul>
<p>所以堆排序的过程主要是构造(初始化)根堆，和弹出最小值之后再构建根堆(重新维持稳定)根堆。初始化等于一个一个元素插入，重新维持相当于从堆中删除元素。<br>所以，主要是两个步骤：插入和删除。</p>
<h3 id="插入节点："><a href="#插入节点：" class="headerlink" title="插入节点："></a>插入节点：</h3><blockquote>
<p>插入到完全二叉树的最后<br>如果有父节点&amp;&amp;父节点比自己小：<br>——-两个节点交换位置</p>
</blockquote>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226151338.png" alt><br>移动次数=层数 所以插入的时间复杂度O(log n)</p>
<h3 id="删除节点："><a href="#删除节点：" class="headerlink" title="删除节点："></a>删除节点：</h3><blockquote>
<p>删除根节点，让末尾最大的值成为新的根节点<br>如果有子节点&amp;&amp;子节点比自己小：<br>——-选择子节点中小的那个，两个节点交换位置</p>
</blockquote>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226151835.png" alt></p>
<p>移动次数=层数 所以删除的时间复杂度O(log n)</p>
<h3 id="完全二叉树和数值的索引对应关系"><a href="#完全二叉树和数值的索引对应关系" class="headerlink" title="完全二叉树和数值的索引对应关系"></a>完全二叉树和数值的索引对应关系</h3><p>父节点 i 找子节点：2i+1 , 2i+2<br>子节点找父节点： [(i-1)/2]   向下取整<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @heapSort</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">heapify</span><span class="params">(nums, n, i)</span>:</span> </span><br><span class="line">         largest = i</span><br><span class="line">         l = <span class="number">2</span> * i + <span class="number">1</span></span><br><span class="line">         r = <span class="number">2</span> * i + <span class="number">2</span></span><br><span class="line">         </span><br><span class="line">         <span class="keyword">if</span> l &lt; n <span class="keyword">and</span> nums[i] &lt; nums[l]: </span><br><span class="line">             largest = l </span><br><span class="line"></span><br><span class="line">         <span class="keyword">if</span> r &lt; n <span class="keyword">and</span> nums[largest] &lt; nums[r]: </span><br><span class="line">             largest = r </span><br><span class="line"></span><br><span class="line">         <span class="keyword">if</span> largest != i: </span><br><span class="line">             nums[i], nums[largest] = nums[largest], nums[i]</span><br><span class="line">             </span><br><span class="line">             heapify(nums, n, largest)</span><br><span class="line">             </span><br><span class="line">     n = len(nums) </span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(n, <span class="number">-1</span>, <span class="number">-1</span>): </span><br><span class="line">         heapify(nums, n, i) </span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): </span><br><span class="line">         nums[i], nums[<span class="number">0</span>] = nums[<span class="number">0</span>], nums[i]</span><br><span class="line">         heapify(nums, i, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="二叉搜索树-BST博客"><a href="#二叉搜索树-BST博客" class="headerlink" title="二叉搜索树 BST博客"></a>二叉搜索树 <a href="https://blog.csdn.net/csdn0123zl/article/details/81253648" target="_blank" rel="noopener">BST博客</a></h1><ul>
<li>Def: 任何一个节点，小于右子节点，大于左子节点<br>条件不够苛刻，二叉搜索树不唯一。</li>
</ul>
<h2 id="节点的后继："><a href="#节点的后继：" class="headerlink" title="节点的后继："></a>节点的后继：</h2><p>大于这个节点的最小的值。<br>分两种情况，如果这个节点x有右子树，那么这个继承者就是右子树的最小值。<br>如果没有右子树，那么就是这个节点一直往上，第一个向右走的父节点。<br>如图。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226160114.png" alt><br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226160138.png" alt></p>
<h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h2><p>插入很简单，就和查找一样，找到一个位置塞进去就好了。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200226160354.png" alt></p>
<h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><p>删除分情况</p>
<ol>
<li>如果0个孩子，直接删</li>
<li>如果1个孩子，用孩子代替这个位置</li>
<li>如果2个孩子，删除孩子的后继，用后继补充这个位置</li>
</ol>
]]></content>
      <categories>
        <category>CSIT5500</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Q音算法实习复盘</title>
    <url>/2020/Q%E9%9F%B3%E7%AE%97%E6%B3%95%E5%AE%9E%E4%B9%A0%E5%A4%8D%E7%9B%98/</url>
    <content><![CDATA[<p>在11月底面试通过（面试参考之前的<a href="https://leeee.top/2019/%E5%87%86%E5%A4%87Q%E9%9F%B3%E6%8E%A8%E8%8D%90%E7%9A%84%E7%AC%94%E8%AE%B0/">准备Q音推荐实习面试的笔记</a>），开始到腾讯音乐推荐组实习，到上周离职，总共历时2个月3周，排除过年放假的两周，工作9周，50天左右。对于我来说，这是第一次技术岗的实习，虽然大二的时候去YY也做过技术岗，不过那个时候就是写SQL，而且也写得不好😂，算不上技术岗。</p><a id="more"></a>
<p>实习结束整体感受良好，感受到了自己的技术薄弱，熟悉了推荐的相关逻辑，有导师每日交流指点，收获还是很大的。“说说技术上的困难吧，其实还是没经验、熟练度低，一个很简单的事情自己可能要想很久做很久，有时可能还会有疏漏，可能前辈一个小时干的事情，我要几个小时甚至好几天，虽然前辈在给我分配任务的时候可能考虑到这个问题，但其实在我看来感觉还是差的太远，非常愧疚，离职返校后现在还在补充基础知识，c++，还有一些建模熟练度的练习，都有坚持在弄。”</p>
<h1 id="氛围-amp-办公环境"><a href="#氛围-amp-办公环境" class="headerlink" title="氛围&amp;办公环境"></a>氛围&amp;办公环境</h1><p>刚去实习的时候，发现每天需要开晨会，每周需要开周会，周会是投屏展示，感觉压力贼大，加上刚入职的几天啥权限都没有，晨会的时候听着正式员工做了balabala，自己没做出来什么，就很尴尬。不过到后来就好了，可能是因为脸厚了🤫。<br>整体的节奏比较快，团队目标明确，一个季度会有一次OKR的会议(类似与KPI)，每周都会review OKR的完成度，好处是我觉得压力大成长快，不好的地方在于，大家都太忙了，专注于自己的okr，团队之间的闲聊较少，遇到各种小问题就不好问，还好有几个实习生同学，大家可以一起讨论细节问题。</p>
<p>办公环境不咋地，尤其是对比WXG。第一次去的时候没有工位，是一张临时的桌子。到后来，搬了工位，有了稍微大一点的空间，还好点。但是地理位置处于腾大旁边，周围都是写字楼，没有生活气息；再加上深圳，尤其是南山区给我的印象就是到处修修建建，感觉噪音比较大，对于环境不太满意。不过之后去了一趟附近的公园，就感觉心灵有了一片栖息地，有所改观。对比在广州的时候，我感觉珠江一直是我的栖息地，夜跑上班都是走珠江边，会心旷神怡。</p>
<h1 id="实习内容"><a href="#实习内容" class="headerlink" title="实习内容"></a>实习内容</h1><p>实习的任务是实现某推荐点位(脱敏)的留存增长，前期主要熟悉现有逻辑，后期上线三个实现，由于逻辑简单，实验也都获得了正向的反馈。🥰<br>（其实“留存增长”对于算法来说，不好直接优化，不是模型输出的直接指标，更没办法写出来loss function，要做的就是完善推荐策略或者模型就好）</p>
<ul>
<li>实验一：对于召回降级池的年龄划分<br>降级池也叫兜底池，其实用户量不大，但是这一块之前没有做细，就留下了空间，我简单的通过年龄的分层就实现了20%的时长增长。年龄分层就可以理解为一层决策树，所以完全可以用树模型训练一版召回，可能是这里用户量不大，前辈也不想做了。<br>涉及工具：主要是SQL定时任务，计算一个新的榜单，线上的rpc服务用之前写好的。</li>
<li>实验二：增加某个召回策略A的比例<br>前期发现某个召回策略的最终表现较好，但是投放量不大。(🤣我第一反应是只采用这一个召回策略，实际上忽略了太多客观因素) 但是前辈解释策略之后，其实就还有增加召回数量的空间。就简单调整了一下策略，开始做C++服务的改进。<br>涉及工具：C++，git（博文<a href="https://leeee.top/2020/Git%E5%86%8D%E5%85%A5%E9%97%A8/">Git再入门</a>），linux（博文<a href="https://leeee.top/2020/linux%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%86%8D%E5%85%A5%E9%97%A8/">linux从入门到再入门</a>）。大一学过C，但是学的也不深；虽然开发逻辑很简单，还是花了不少时间。</li>
<li>实验三：增加排序的图像特征<br>我只做了离线的验证图像特征有效性。图像特征当然不是我自己跑的啦，准备好数据集，发送给对应做CV的同学，然后用训练好的模型返回数据。拼接到现有的数据集上，跑了xgboost和RankNet两个模型，最后还真的有效果，auc有客观的提升。不错不错。🤤<br>涉及工具：python，拼接特征，然后大概看了看模型的结构。</li>
</ul>
<p>整体来说熟悉开发环境，申请各种权限，了解现有逻辑，熟悉别人代码，占据了主要的时间。😢</p>
<h1 id="杂乱的感悟"><a href="#杂乱的感悟" class="headerlink" title="杂乱的感悟"></a>杂乱的感悟</h1><p>由于思考的过程都很琐碎，就不刻意串起来了，简单罗列。</p>
<p>算法方面：</p>
<ul>
<li>模型、特征的可解释性决定了算法的天花板。如果模型对于我们来说是一个黑匣子，那就只能当”炼丹师“，盲目尝试。这也需要对模型有更深刻的理解。</li>
<li>召回中，策略相对于模型来说，有更好的解释性，也有更好的投放空间，但是个性化推荐不上模型效果往往不佳；建议外层框架用策略，具体召回必须上模型；</li>
<li>排序中，一般来说，加入的特征越多，效果越好；比如专辑封面图都会影响用户对于歌曲的喜好；</li>
<li>对于排序召回的组合推荐算法来说：排序保证了下限，召回决定上限；</li>
</ul>
<p>分析方面：</p>
<ul>
<li>分析数据的时候，每进行下一步操作之前，尽量先给出假设，“如果出现这种情形，我有什么对策”，不然会陷入无意义的数据中，数据不会开口说话告诉你答案。</li>
<li>badcase，要分清楚是个人需求还是群体需求，从而决定优先级；</li>
<li>大部分的算法岗都是结合业务的算法应用，真正硬核的发论文的不多，不要太畏惧。所以懂业务逻辑就很重要，不理解业务就很难洞察到能继续优化的点；eg：转发行为如何区分正负态度；如何衡量不同指标的关系，如完播率和收藏率之间；抖音神曲适不适合适不适合做推荐？不同点位之间的差异（激进还是保守？多样还是资产？）</li>
<li>一旦洞察到优化的点之后，往往简单的逻辑就能造成很好的改善，不一定是很fancy的模型；eg：如果发现用户对于资产的新旧敏感，缩短召回的时间窗口就可以带来很好的效果；</li>
</ul>
<p>其他：</p>
<ul>
<li>每个人有不同的代码风格，但是一定要确保规范：缩进一致、变量命名有意义、备注好解释（防止日后自己都看不懂）；</li>
<li>老板不需要知道怎么使用工具，但是需要懂什么工具能解决什么问题(我遇到的老板知识面很广)；</li>
<li>导师对你严厉很有可能是故意，大概是防止你什么细节都直接过问吧😂，一般大家为人都很好；</li>
</ul>
<p>最后感谢实习期间的导师，让我受益匪浅。</p>
<p>如果你觉得读完有一丢丢用处，就在下面留个言吧👀</p>
]]></content>
      <tags>
        <tag>实习</tag>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n-3 回顾神经网络，文本分类介绍</title>
    <url>/2020/CS224n-3/</url>
    <content><![CDATA[<p>本节课主要是：<br>分类器回顾，神经网络回顾，NER  Named Entity Recognition，窗口词分类，矩阵计算回顾。总体是回顾旧知识，以及nlp任务的举例，快速过一下。</p><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>分类问题很常见了，对于NLP来说：</p><a id="more"></a>

<ul>
<li>x ： 单词，句子，文章</li>
<li>y ：情感，是否NER，决策判别，是否人话</li>
</ul>
<h2 id="回顾softmax："><a href="#回顾softmax：" class="headerlink" title="回顾softmax："></a>回顾softmax：</h2><script type="math/tex; mode=display">
p(y | x)=\frac{\exp \left(W_{y} \cdot x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}</script><p>分子：输入向量和矩阵的一行相乘<br>分母：所有相乘，归一化</p>
<p>训练可以直接最大化概率，或者最小化负log概率。</p>
<script type="math/tex; mode=display">p(y_j = 1|x) = \frac{\exp(W_{j\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)}</script><p>但是，巧合的是，最小化负log概率对于softmax等同于cross entropy。</p>
<blockquote>
<p> 回顾cross entropy：<br>交叉熵来源于信息论，对于对于c情况真实概率p，模型计算的概率q，那么交叉熵计算为：$H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)$</p>
</blockquote>
<p>对于softmax的交叉熵来说：</p>
<script type="math/tex; mode=display">
\begin{aligned} H(\hat{y}, y) &=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right) \\ &=-\sum_{j=1}^{C} y_{j} \log \left(p\left(y_{j}=1 | x\right)\right) \\ &=-\sum_{j=1}^{C} y_{j} \log \left(\frac{\exp \left(W_{j}, x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}\right) \\ &=-y_{i} \log \left(\hat{y}_{i}\right) \end{aligned}</script><p>最后一个等号的求和去掉是因为，只有当$y_i$预测对了，才会累加，否则都是0.</p>
<p>所以，交叉熵和负对数概率是等价的。</p>
<p>对于有N个样本的数据集，整体的损失就是：</p>
<script type="math/tex; mode=display">
-\sum_{i=1}^{N} \log \left(\frac{\exp \left(W_{k(i)} \cdot x^{(i)}\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x^{(i)}\right)}\right)</script><blockquote>
<p>括号里就是对于每一个样本x的预测值概率大小，所有都猜对的话，概率是1，整体损失就是0；当没猜对，概率在0-1，-log(0. xxx)就是很小，损失就多。</p>
</blockquote>
<p>一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222013518.png" alt></p>
<h1 id="神经元的本质"><a href="#神经元的本质" class="headerlink" title="神经元的本质"></a>神经元的本质</h1><p>如果损失函数用sigmoid，单个神经元就是逻辑回归：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222014539.png" alt></p>
<p>而神经网络，就是多个逻辑回归同时计算，如图：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222014651.png" alt><br>上图就是四个逻辑回归(经过4次激活函数)</p>
<h1 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h1><p>实体识别就是把句子里的实物名字找出来，并且进行分类，比如说组织单位，个人名字，电器名字，软件名字，地点之类的。<br>用途：</p>
<ul>
<li>定位前文的主语</li>
<li>问答系统需要实体</li>
<li>信息一般都与实体相关</li>
<li>slot-filling 填空问题</li>
</ul>
<p>由于自然语言中，大部分实体模糊不清，代词也会干扰，所以不简单。</p>
<blockquote>
<p>实体识别就是把句子里是实物找出来</p>
</blockquote>
<h1 id="窗口单词分类Binary-word-window-classification"><a href="#窗口单词分类Binary-word-window-classification" class="headerlink" title="窗口单词分类Binary word window classification"></a>窗口单词分类Binary word window classification</h1><p>单独对于一个单词分类很难，所以简单思路就是取前后的四五个单词窗口，取平均（或者把向量串起来），然后再进行分类。</p>
<p>但是有一个问题，平均会丢失单词在窗口里的位置。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222214054.png" alt></p>
<p>对于位置问题，可以通过打分来解决（回归）。让位置在中间的样本y高，不在中间的样本（打乱顺序）对应的yscore低。如图。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222214634.png" alt></p>
<h1 id="间隔最大化目标函数"><a href="#间隔最大化目标函数" class="headerlink" title="间隔最大化目标函数"></a>间隔最大化目标函数</h1><p>这里上课老师跳过了，没看懂先挖个坑<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222215915.png" alt></p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>求导，链式法则，老生常谈了。</p>
<p>关注一下矩阵的求导：</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222220647.png" alt></p>
<p>对于一个m输出，n输入的函数，求导就是对于m行，每行求对应位置输入x_j的导数。</p>
<p>n个输入n个输出：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222221411.png" alt><br>所以，可以得到一个只有对角线元素上有导数，其他位置为0的矩阵。</p>
<p>所以，最后可以得到s（最后的分数）对b(线性计算加的那个偏置)的导数：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222222311.png" alt></p>
<p>上图中，如果同时对w和b求导可以发现：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222222558.png" alt></p>
<p>观察出，b的导数就是信号的损失</p>
<p>上图第一行，z对w的求导就是x。所以可以得到s对于w的导数，其实就是delt损失和x相乘。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222223106.png" alt></p>
<p>所以导数也是n*m的。</p>
<blockquote>
<p>这里没讲完就下课了…，主要是想展示计算过程吧。</p>
</blockquote>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n-2 word2vec, word Senses</title>
    <url>/2020/CS224n-2-word2vec-word-Senses/</url>
    <content><![CDATA[<h1 id="CS224n-2-word2vec-word-Senses"><a href="#CS224n-2-word2vec-word-Senses" class="headerlink" title="CS224n-2 word2vec, word Senses"></a>CS224n-2 word2vec, word Senses</h1><p>这节课我不按课堂讲的，引用一篇<a href="https://www.jianshu.com/p/a6bc14323d77" target="_blank" rel="noopener">博客</a></p><h1 id="word-vectors-and-word2vec"><a href="#word-vectors-and-word2vec" class="headerlink" title="word vectors and word2vec"></a>word vectors and word2vec</h1><h2 id="代表技术之一-word2vec"><a href="#代表技术之一-word2vec" class="headerlink" title="代表技术之一 word2vec"></a>代表技术之一 word2vec</h2><p>2013年，Google团队发表了word2vec工具 [1]。word2vec工具主要包含两个模型：跳字模型（skip-gram）和连续词袋模型（continuous bag of words，简称CBOW），以及两种近似训练法：负采样（negative sampling）和层序softmax（hierarchical softmax）。值得一提的是，word2vec的词向量可以较好地表达不同词之间的相似和类比关系。</p><a id="more"></a>

<p>word2vec自提出后被广泛应用在自然语言处理任务中。它的模型和训练方法也启发了很多后续的词嵌入模型。本节将重点介绍word2vec的模型和训练方法。</p>
<h2 id="Skip-gram模型（跳字模型）："><a href="#Skip-gram模型（跳字模型）：" class="headerlink" title="Skip-gram模型（跳字模型）："></a>Skip-gram模型（跳字模型）：</h2><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208213903.png" alt><br>Skip-gram</p>
<p>在跳字模型中，我们用一个词来预测它在文本序列周围的词。</p>
<p>举个例子，假设文本序列是：</p>
<blockquote>
<p>“I love you very much”</p>
</blockquote>
<p>跳字模型所关心的是，给定“<strong>you</strong>”生成邻近词“I”、“love”、“very”和“much”的条件概率。</p>
<p>在这个例子中，“you”叫中心词，“I”、“love”、“very”和“much”叫背景词。</p>
<p>由于“you”只生成与它距离不超过2的背景词，该<strong>时间窗口的大小为2</strong>[与N-gram类似]。</p>
<p>我们来描述一下跳字模型[用最大似然估计的思想]：</p>
<p>假设词典索引集V的大小为|V|，且{0,1,…,|V|−1}。给定一个长度为T的文本序列中，文本序列中第t的词为w(t)。当时间窗口大小为m时，跳字模型需要<strong>最大化给定任一中心词生成所有背景词的概率：</strong></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208213949.png" alt></p>
<p>上式的<strong>最大似然估计</strong>与<strong>最小化以下损失函数</strong>等价：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1803066-6089ead7ce1ea503.png?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp" alt></p>
<p>我们可以用<strong>v</strong>和<strong>u</strong>分别表示 <strong>中心词</strong> 和 <strong>背景词</strong> 的向量。</p>
<p>换言之，对于词典中索引为i的词，它在作为中心词和背景词时的向量表示分别是vi和ui。而词典中所有词的这两种向量正是跳字模型所要学习的模型参数。为了将模型参数植入损失函数，我们需要使用模型参数表达损失函数中的给定中心词生成背景词的条件概率。给定中心词，假设生成各个背景词是相互独立的。设中心词wc在词典中索引为c，背景词wo在词典中索引为o，损失函数中的给定中心词生成背景词的<strong>条件概率</strong>可以通过softmax函数定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1803066-69d819ac1e385080.png?imageMogr2/auto-orient/strip|imageView2/2/w/662/format/webp" alt></p>
<blockquote>
<p>上式：给定任何一个中心词Wc，产生背景词Wo的概率</p>
<p>每一个词，在模型中有两个词向量，一个是作为中心词时的词向量，一个是作为背景词时的词向量</p>
</blockquote>
<p><strong>利用随机梯度下降求解：</strong></p>
<p>当序列长度T较大时，我们通常在每次迭代时随机采样一个较短的子序列来计算有关该子序列的损失。然后，根据该损失计算词向量的梯度并迭代词向量。具体算法可以参考<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fzh.gluon.ai%2Fchapter_optimization%2Fgd-sgd-scratch.html" target="_blank" rel="noopener">“梯度下降和随机梯度下降——从零开始”</a>一节。 作为一个具体的例子，下面我们看看如何计算随机采样的子序列的损失有关中心词向量的梯度。和上面提到的长度为T的文本序列的损失函数类似，随机采样的子序列的损失实际上是对子序列中给定中心词生成背景词的条件概率的对数求平均。通过微分，我们可以得到上式中条件概率的对数有关中心词向量vc的<strong>梯度：</strong>  </p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215618.png" alt></p>
<p><strong>该式也可改写作：</strong></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215642.png" alt></p>
<blockquote>
<p>上面的迭代更新计算开销太大！！每次都需要遍历整个字典，对应的解决方案在后面（这也是word2vec为啥这么牛逼的原因…厉害的不是这个工具本身，而是一种思想的应用）</p>
</blockquote>
<p>随机采样的子序列有关其他词向量的梯度同理可得。训练模型时，每一次迭代实际上是用这些梯度来迭代子序列中出现过的中心词和背景词的向量。训练结束后，对于词典中的任一索引为i的词，我们均得到该词作为中心词和背景词的两组词向量vi和ui。在自然语言处理应用中，我们会使用跳字模型的中心词向量。</p>
<h4 id="求梯度过程"><a href="#求梯度过程" class="headerlink" title="求梯度过程"></a>求梯度过程</h4><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208220835.png" alt></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208220909.png" alt><br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208220919.png" alt><br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208220926.png" alt></p>
<hr>
<h3 id="CBOW-连续词袋模型"><a href="#CBOW-连续词袋模型" class="headerlink" title="CBOW(连续词袋模型)"></a>CBOW(连续词袋模型)</h3><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215727.png" alt></p>
<p>CBOW</p>
<p>连续词袋模型与跳字模型类似,与跳字模型最大的不同是：</p>
<p>连续词袋模型用一个中心词在文本序列周围的词来预测该中心词。</p>
<p>举个例子，假设文本序列为：</p>
<blockquote>
<p>“I love you very much”</p>
</blockquote>
<p>连续词袋模型所关心的是，邻近词“I”、“love”、“very”和“much”一起生成中心词“you”的概率。</p>
<p>假设词典索引集的大小为V，且V={0,1,…,|V|−1}&lt;/nobr&gt;。给定一个长度为T的文本序列中，文本序列中第t个词为wu(t)。当时间窗口大小为m时，连续词袋模型需要最大化由背景词生成任一中心词的概率</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215738.png" alt></p>
<p>上式的最大似然估计与最小化以下损失函数等价：</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215749.png" alt></p>
<p>我们可以用<strong>v</strong>和<strong>u</strong>分别表示背景词和中心词的向量（注意符号和跳字模型中的不同）。换言之，对于词典中索引为i的词，它在作为背景词和中心词时的向量表示分别是vi和ui。而词典中所有词的这两种向量正是连续词袋模型所要学习的模型参数。为了将模型参数植入损失函数，我们需要使用模型参数表达损失函数中的给定背景词生成中心词的概率。设中心词wc在词典中索引为c，背景词wo1、wo2、…wo2m在词典中索引为o1、o2、….o2m-1、o2m，损失函数中的给定背景词生成中心词的概率可以通过softmax函数定义为</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208215805.png" alt></p>
<p>和跳字模型一样，当序列长度T较大时，我们通常在每次迭代时随机采样一个较短的子序列来计算有关该子序列的损失。然后，根据该损失计算词向量的梯度并迭代词向量。 通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量voi(i=1,2,….2m)的梯度为：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208221019.png" alt></p>
<p>该式也可写作</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208221029.png" alt></p>
<p>随机采样的子序列有关其他词向量的梯度同理可得。和跳字模型一样，训练结束后，对于词典中的任一索引为i的词，我们均得到该词作为背景词和中心词的两组词向量vi和ui。<br>在自然语言处理应用中，我们会使用连续词袋模型的背景词向量。</p>
<hr>
<h1 id="近似训练法"><a href="#近似训练法" class="headerlink" title="近似训练法"></a>近似训练法</h1><p>我们可以看到，无论是skip-gram(跳字模型)还是CBOW(连续词袋模型)，每一步梯度计算的开销与词典V的大小相关。</p>
<blockquote>
<p>因为计算softmax的时考虑了字典上的所有可能性</p>
</blockquote>
<p>当词典较大时，例如几十万到上百万，这种训练方法的计算开销会较大。因此，我们将使用近似的方法来计算这些梯度，从而减小计算开销。常用的近似训练法包括负采样和层序softmax。</p>
<h2 id="1-负采样（Negative-Sample）"><a href="#1-负采样（Negative-Sample）" class="headerlink" title="(1)负采样（Negative Sample）"></a>(1)负采样（Negative Sample）</h2><p>我们以跳字模型为例讨论负采样。</p>
<p>实际上，词典V的大小之所以会在损失中出现，是因为给定中心词$w_c$生成背景词wo的条件概率$P(w_0∣w_c)$</p>
<blockquote>
<p>使用了softmax运算，而softmax运算正是<strong>考虑了背景词可能是词典中的任一词（使用了全部词）</strong>，并体现在分母上。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224459.png" alt></p>
</blockquote>
<p>下面，我们可以使用σ(x)=1/(1+exp(−x))函数来表达中心词wc和背景词wo同时出现在该训练数据窗口的概率。</p>
<blockquote>
<p>σ(x)属于[0,1]<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224516.png" alt></p>
</blockquote>
<p>那么，给定中心词wc生成背景词wo的条件概率的对数可以近似为:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224525.png" alt></p>
<p>[上式的含义：中心词wc与背景词wo同时出(D=1)现概率，且中心词wc与噪音词wk不同时出现(D=0)的概率。]</p>
<p>假设噪声词wk在词典中的索引为ik，上式可改写为:</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224609.png" alt></p>
<p>因此，有关给定中心词wc生成背景词wo的损失是:</p>
<p>假设词典V很大，每次迭代的计算开销由O(|V|)变为O(|K|)。当我们把K取较小值时，负采样每次迭代的计算开销将较小。</p>
<p>当然，我们也可以对连续词袋模型进行负采样。有关给定背景词<br>wt-m、wt-m+1、…、wt+m生成中心词wc的损失:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224615.png" alt></p>
<p>在负采样中可以近似为:</p>
<p>  <img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224627.png" alt></p>
<p>同样，当我们把K取较小值时，负采样每次迭代的计算开销将较小。</p>
<h2 id="2-层序softmax"><a href="#2-层序softmax" class="headerlink" title="(2)层序softmax[]"></a>(2)层序softmax[]</h2><p>层序softmax是另一种常用的近似训练法。它利用了二叉树这一数据结构。树的每个叶子节点代表着词典V中的每个词。  </p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224641.png" alt></p>
<p>假设L(w)为从二叉树的根节点到词w&lt;的叶子节点的路径（包括根和叶子节点）上的节点数。设n(w,j)为该路径上第j个节点，并设该节点的向量为un(w,j)。以上图为例：L(w3)=4。设词典中的词wi的词向量为vi。那么，跳字模型和连续词袋模型所需要计算的给定词wi生成词w的条件概率为：<img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224651.png" alt><br>其中σ(x)=1/(1+exp(−x))，leftChild(n)是节点n的左孩子节点，如果判断x为真，[x]=1；反之[x]=−1。由于σ(x)+σ(−x)=1，给定词wi生成词典V中任一词的条件概率之和为1这一条件也将满足：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224729.png" alt></p>
<p>让我们计算给定词wi生成词w3的条件概率。我们需要将wi的词向量vi和根节点到w3路径上的非叶子节点向量一一求内积。由于在二叉树中由根节点到叶子节点w3的路径上需要向左、向右、再向左地遍历，我们得到:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208224736.png" alt></p>
<blockquote>
<p><strong>整个遍历的路径已经通过Huffman编码唯一的确定了</strong></p>
</blockquote>
<p>在使用softmax的跳字模型和连续词袋模型中，词向量和二叉树中非叶子节点向量是需要学习的模型参数。</p>
<p>假设词典V很大，每次迭代的计算开销由O(|V|)下降至O(log2|V|)。</p>
<p>推荐资料：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fx-hacker%2FWordEmbedding" target="_blank" rel="noopener">学习word2vec的经典资料</a></p>
<h1 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208225228.png" alt></p>
<p>分解矩阵，就可以得到embedding，和推荐系统的思路一样。SVD也存在维数问题，以及矩阵分解的困难。</p>
<h1 id="GloVe算法-基于局部和全局"><a href="#GloVe算法-基于局部和全局" class="headerlink" title="GloVe算法-基于局部和全局"></a>GloVe算法-基于局部和全局</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208225801.png" alt><br>比较SVD这种count based模型与Word2Vec这种direct prediction模型，它们各有优缺点：Count based模型优点是训练快速，并且有效的利用了统计信息，缺点是对于高频词汇较为偏向，并且仅能概括词组的相关性，而且有的时候产生的word vector对于解释词的含义如word analogy等任务效果不好；Direct Prediction优点是可以概括比相关性更为复杂的信息，进行word analogy等任务时效果较好，缺点是对统计信息利用的不够充分。所以Manning教授他们想采取一种方法可以结合两者的优势，并将这种算法命名为GloVe（Global Vectors的缩写），表示他们可以有效的利用全局的统计信息。</p>
<p>那么如何有效的利用word-word co-occurrence count并能学习到词语背后的含义呢？首先为表述问题简洁需要，先定义一些符号：对于矩阵X，$X_{ij}$代表了单词 i 出现在单词 j 上下文中的次数，则$X_i=\sum_k X_{ij}$<br> 即代表所有出现在单词 i 的上下文中的单词次数。我们用$P_{i j}=P(j | i)=\frac{X_{i j}}{X_{i}}$来代表单词 j 出现在单词 i 上下文中的概率。</p>
<p>我们用一个小例子来解释如何利用co-occurrence probability来表示词汇含义：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208225816.png" alt><br>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词 [公式] 的co-occurrence probability 的比值来描述，例如对于solid固态，虽然 $P(solidice)$ 与 $P(solid∣steam)$本身很小，不能透露有效的信息，但是它们的比值$\frac{P(solid|ice)} {P(solid|steam)}$ 却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大，对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义。</p>
<p>通过定义如下的损失函数：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208230330.png" alt></p>
<p>优点是训练快，小数据集表现好。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208233632.png" alt></p>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n-1 初步探索，背景介绍  </title>
    <url>/2020/CS224n-1-%E5%88%9D%E6%AD%A5%E6%8E%A2%E7%B4%A2%EF%BC%8C%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>NLP持续升温，作为入门课程，224n声名远扬。<br>听了10节课之后，果然感觉不愧是斯坦福。<br>这门课逻辑很清晰（遇到什么问题，产生什么思路，做了什么探索模型，怎么解决问题），而不是直接的扔出来一个模型，之前遇到很多老师这样，让人摸不到头脑；这门课会讲如何做一个研究以及如何写一篇paper。这些虽然不是NLP必备，是一些普世的知识，但是课堂上不讲，学生也很难完全了解到，基本上都、是自己摸索。</p><a id="more"></a>
<p>总的来说，觉得相见恨晚，虽然有点难，但是学的很舒服！现在开始整理每节课的笔记。</p>
<hr>
<p>Lecture 1: Introduction and Word Vectors 第一节课主要是介绍背景</p>
<h1 id="The-course-10-mins"><a href="#The-course-10-mins" class="headerlink" title="The course (10 mins)"></a>The course (10 mins)</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>自然语言处理（NLP）</strong>是计算科学、AI、语言学的交叉学科，其目的是为了让机器能够处理或者是“理解”自然语言，比如可以通过说话让机器为我们定会议、买东西。机器完全理解是不大可能的，完全理解基本等同于AI完全实现了。</p>
<h2 id="如何表示词义？"><a href="#如何表示词义？" class="headerlink" title="如何表示词义？"></a>如何表示词义？</h2><ul>
<li><p>Def：meaning：就是人写的，说的，短语句子符号来表达的想法</p>
<h2 id="如何用计算机表达词义？"><a href="#如何用计算机表达词义？" class="headerlink" title="如何用计算机表达词义？"></a>如何用计算机表达词义？</h2><h3 id="WordNet："><a href="#WordNet：" class="headerlink" title="WordNet："></a>WordNet：</h3><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208165156.png" alt></p>
<p>WordNet是人为的收录了近义词和相似词。<br>存在的问题是：没办法统计到细微差别；没有新词；需要人为的统计输入；不能计算相似度；依赖生成词库的主观。</p>
</li>
</ul>
<h3 id="把词当做离散变量one-hot"><a href="#把词当做离散变量one-hot" class="headerlink" title="把词当做离散变量one-hot"></a>把词当做离散变量one-hot</h3><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208165732.png" alt></p>
<p> 存在的问题是：词库长度太大；不能计算相似度；</p>
<h3 id="通过上下文表示"><a href="#通过上下文表示" class="headerlink" title="通过上下文表示"></a>通过上下文表示</h3><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208170015.png" alt><br>通过上下文的思路，可以引出主流方法Word vectors（word embeddings or word representations）。<br>每一个词对应一个N维度的向量，N一般几百维。</p>
<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>从2013年开始有了Word2vec，如何生成向量的方法逐渐出现。</p>
<p>思路：</p>
<ul>
<li>有一个大的语料库</li>
<li>每个词都表示成向量</li>
<li>遍历每个词，中间的词当做c，周围的几个当做o</li>
<li>通过计算每个词向量和周围词向量出现的概率</li>
<li>调整词向量，让概率最大</li>
</ul>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208171649.png" alt></p>
<p>目标函数：</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208171826.png" alt></p>
<p>理解：这里就是对于语料(Data)的每个词t，对应前后j位置的词，是词a和词b的概率P，都乘起来。（这里还是很有统计色彩的)</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200208172914.png" alt><br>注意：这里需要维护两套向量，一套是中间的，一套是周围的。</p>
<p>其他的都是一些微积分和优化的知识，不做记录了。</p>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>FM,FFM,DeepFM </title>
    <url>/2020/FM-FFM%E4%B8%8E%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>最近过年在家因为疫情不能外出，随手抓了本推荐系统开始看。模型部分从传统的邻域（协同过滤）到隐语义模型（LFM）到矩阵分解模型（SVD，SVD++），FM和FFM等遇到颇多问题，在此梳理一下。</p><h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><p>FM的paper地址如下：<a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf</a><br>FM主要目标是：解决数据稀疏的情况下，特征怎样组合的问题<br>根据paper的描述，FM有一下三个优点：</p><a id="more"></a>

<ol>
<li>处理稀疏数据</li>
<li>FM模型的时间复杂度是线性的</li>
<li>FM是一个通用模型，它可以用于任何特征为实值的情况</li>
</ol>
<ul>
<li>一般的线性模型：<br>$y=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}$</li>
</ul>
<p>从上面的式子中看出，一般的线性模型没有考虑特征之间的关联。</p>
<ul>
<li>为了简化，采用最简单的<strong>二阶组合+多项式相乘</strong>作为关联特征的例子。<br>$y=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}$</li>
</ul>
<p>该多项是模型与线性模型相比，多了特征组合的部分，特征组合部分的参数有$\frac{n(n-1)}{2}$个($w_{i j}$是对称的)。如果特征非常稀疏且维度很高的话，时间复杂度O(N^2)。<br>所以，通过引入辅助向量lantent vector $V_{i}=\left[v_{i 1}, v_{i 2}, \dots, v_{i k}\right]^{T}$来表示$\frac{n(n-1)}{2}$个参数。（这里借用了FunkSVD(LFM，Latent Factor Model)的思想）    </p>
<ul>
<li>FM<br>$y=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}<v_{i}, v_{j}>x_{i} x_{j}$</v_{i},></li>
</ul>
<p>这里，辅助向量V的长度K一般是远远小于n的，所以参数量从$\frac{n(n-1)}{2}$个变成了$nk$，参数量降低，而且每一个向量V也表示了x的一个维度，有更好的解释性，<strong>本质上是在对特征进行embedding</strong>。且时间复杂度降为O(kN).</p>
<p>时间复杂度化简后方可看出：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200129213611.png" alt></p>
<h1 id="FFM-Field-aware-Factorization-Machines"><a href="#FFM-Field-aware-Factorization-Machines" class="headerlink" title="FFM(Field-aware Factorization Machines)"></a>FFM(Field-aware Factorization Machines)</h1><p>FFM是FM的升级版模型，引入了field的概念。FFM把相同性质的特征归于同一个field。在FFM中，每一维特征$x_i$，针对每一种field $f_j$，都会学习到一个隐向量$V_{i,f_j}$，因此，隐向量不仅与特征相关，也与field相关。</p>
<p>设样本一共有n个特征, f 个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看做FFM的特例，即把所有特征都归属到同一个field中。</p>
<p>$y = w_0 + \Sigma_{i=1}^{n}w_{i} x_{i} + \Sigma_{i=1}^{n}\Sigma_{j=i+1}^{n}<v_{i,f_{j}}, v_{j,f_{i}}>x_{i}x_{j}$</v_{i,f_{j}},></p>
<p>如果隐向量的长度为k，那么FFM的二次项参数数量为nfk，远多于FM模型。此外由于隐向量与field相关，FFM二次项并不能够化简，时间复杂度为$O(kn^2)$。需要注意的是由于FFM中的latent vector只需要学习特定的field，所以通常$K_{FFM} &lt;&lt; K_{FM}$。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200130123459.png" alt></p>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><p>当理解了FM，deepfm的思路也就很好懂了。</p>
<p>fm是低阶特征，dnn是高阶特征，把低阶和高阶特征并行就是deepFM<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200130132900.png" alt><br>我们先来看一下DeepFM的模型结构：</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200130135729.png" alt><br>FM部分是一个因子分解机。关于因子分解机可以参阅文章[Rendle, 2010] Steffen Rendle. Factorization machines. In ICDM, 2010.。因为引入了隐变量的原因，对于几乎不出现或者很少出现的隐变量，FM也可以很好的学习。<br><strong>深度部分</strong><br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200130135817.png" alt></p>
<p>深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。</p>
<p>  <img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200130135833.png" alt><br>  嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性，1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2)在FM里得到的隐变量Vik现在作为了嵌入层网络的权重。</p>
<p>这里的第二点如何理解呢，假设我们的k=5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。</p>
<p>代码地址：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FChenglongChen%2Ftensorflow-DeepFM" target="_blank" rel="noopener">https://github.com/ChenglongChen/tensorflow-DeepFM</a></p>
<p>梳理FM发展历史：<a href="https://zhuanlan.zhihu.com/p/52877868" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/52877868</a><br>参考FM：<a href="https://blog.csdn.net/John_xyz/article/details/78933253" target="_blank" rel="noopener">https://blog.csdn.net/John_xyz/article/details/78933253</a><br>带举例的解释：<a href="https://blog.csdn.net/baymax_007/article/details/83931698" target="_blank" rel="noopener">https://blog.csdn.net/baymax_007/article/details/83931698</a><br>推荐系统遇上深度学习系列1-3：<a href="https://www.jianshu.com/p/6f1c2643d31b" target="_blank" rel="noopener">https://www.jianshu.com/p/6f1c2643d31b</a></p>
]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>2020香港马拉松备赛</title>
    <url>/2020/2020%E9%A6%99%E6%B8%AF%E9%A9%AC%E6%8B%89%E6%9D%BE%E5%A4%87%E8%B5%9B/</url>
    <content><![CDATA[<p>2020香港马拉松倒计时：</p><p>
    <div style="width:100%; height:50px;border:none;text-align:center">
    <center><iframe allowtransparency="yes" frameborder="0" src="/time.html"></iframe></center>
    </div>
</p><hr><p>备赛日记</p><h1 id="1月26日"><a href="#1月26日" class="headerlink" title="1月26日"></a>1月26日</h1><p>因为肺炎，大赛取消。 渣马有缘再见。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200126115659.png" alt></p><h1 id="1月14日"><a href="#1月14日" class="headerlink" title="1月14日"></a>1月14日</h1><p>早上跑了5公里，公园高低起伏，上坡下坡很不舒服。速度也提不上来，就当是第一次热身了。</p><a id="more"></a>







<h1 id="1月13日"><a href="#1月13日" class="headerlink" title="1月13日"></a>1月13日</h1><p>周一开完组会已经11点了，放弃了。</p>
<h1 id="1月12日-发现场地，初步热身，开始备赛"><a href="#1月12日-发现场地，初步热身，开始备赛" class="headerlink" title="1月12日 发现场地，初步热身，开始备赛"></a>1月12日 发现场地，初步热身，开始备赛</h1><p>我是大二的时候开始热爱运动的，当时也奇怪，总想热爱点什么。没想到跑步坚持了一年多，才放弃。当时不知道哪来的热情，六七点起床去跑珠江边，一跑就是十来公里，上头的时候早上8点的课，我也要6点去跑步。现在真的没有那么兴奋了233333。当时的想法就是参加一次马拉松证明一下自己。本科四年，报名了三次广州马拉松的半马，结果三次都没有中签。害，现在跑个步都要排队的吗？到之后兴趣也就慢慢减弱了。</p>
<p>而打败跑步的是另一个热爱，健身。连续去了健身房两年，一周三次以上，让我学习了几乎所有b站建设up主的教程，在学校的英东健身房也算是“有头有脸”的人。认识了许多小老弟。</p>
<p>害，扯远了。上学期在香港，看到香港马拉松的报名，毫不犹豫的直接报名了。时间是2月8号，也就是开学后一天。离现在也只有不到一个月的时间，近几个月我好吃懒做，加班严重，公司附近也没有很好的健身房(其实是没有去找)。反正就是心肺都下降了很多，力量也弱，六七个引体都做不了(去年还能一次30个呢，哼)。</p>
<p>所以，接下来三周，我要进入疯狂备赛阶段，今晚去公司对面的公园，发现环境还不错，面积挺大，一圈又两公里左右。有很多跑步的人和健身老头。所以我可以接下来一周(11-19号)就在公园锻炼，然后过年回家十来天，在家的体育场，年后(2月1号-7号)再来一周公园锻炼，就要去跑马拉松啦。</p>
]]></content>
  </entry>
  <entry>
    <title>Git再入门</title>
    <url>/2020/Git%E5%86%8D%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="认识多年，但我还是不懂你—GIT"><a href="#认识多年，但我还是不懂你—GIT" class="headerlink" title="认识多年，但我还是不懂你—GIT"></a>认识多年，但我还是不懂你—GIT</h1><p>虽然用github已经很多年了，但是git的各种命令没怎么接触过，能用的也只是简单的clone和pull，也没用真正的理解git的内涵。作为多人合作和版本控制的工具，git强大的一面我还没有接触到。</p><a id="more"></a>
<p>最近实习的时候需要编写cpp服务，写好了就需要git push 到新的分支去，等待导师code review，然后merge到master，中间遇到很多坑，在此就一篇文章整理完所有git方面的知识。</p>
<p>刚开始接触git会遇到很多命令，记不住没关系，先花两个小时理解git的概念，之后遇到问题再查。<br>详细教程可以看：<a href="https://www.runoob.com/git/git-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/git/git-tutorial.html</a></p>
<h1 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h1><p>版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术。</p>
<ul>
<li>实现跨区域多人协同开发</li>
<li>追踪和记载一个或者多个文件的历史记录</li>
<li>组织和保护你的源代码和文档</li>
<li>统计工作量</li>
<li>并行开发、提高开发效率</li>
<li>跟踪记录整个软件的开发过程</li>
<li>减轻开发人员的负担，节省时间，同时降低人为错误</li>
</ul>
<p><strong>简单说就是用于管理多人协同开发项目的技术。</strong></p>
<h1 id="git的基本概念"><a href="#git的基本概念" class="headerlink" title="git的基本概念"></a>git的基本概念</h1><h2 id="工作区"><a href="#工作区" class="headerlink" title="工作区"></a>工作区</h2><p>Git本地有四个工作区域：工作目录（Working Directory）、暂存区(Stage/Index)、资源库(Repository或Git Directory)、git仓库(Remote Directory)。文件在这四个区域之间的转换关系如下：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200111202138.png" alt><br><strong>Workspace</strong>： 工作区，就是你平时存放项目代码的地方<br><strong>Index / Stage</strong>： 暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息<br><strong>Repository</strong>： 仓库区（或版本库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本<br><strong>Remote</strong>： 远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换</p>
<h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ol>
<li>clone项目或者创建项目Repository</li>
<li>写代码</li>
<li>git add 修改了的有价值的代码到index</li>
<li>index commit提交到本地的项目Repository</li>
<li>本地改好了就上传到服务器上remote</li>
</ol>
<h1 id="文件的四种状态"><a href="#文件的四种状态" class="headerlink" title="文件的四种状态"></a>文件的四种状态</h1><p>版本控制就是对文件的版本控制，要对文件进行修改、提交等操作，首先要知道文件当前在什么状态，不然可能会提交了现在还不想提交的文件，或者要提交的文件没提交上。</p>
<p>GIT不关心文件两个版本之间的具体差别，而是关心文件的整体是否有改变，若文件被改变，在添加提交时就生成文件新版本的快照，而判断文件整体是否改变的方法就是用SHA-1算法计算文件的校验和。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200111202617.png" alt><br><strong>Untracked:</strong> 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过git add 状态变为Staged.</p>
<p><strong>Unmodify:</strong> 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为Modified. 如果使用git rm移出版本库, 则成为Untracked文件</p>
<p><strong>Modified:</strong>  文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过git add可进入暂存staged状态, 使用git checkout 则丢弃修改过,返回到unmodify状态, 这个git checkout即从库中取出文件, 覆盖当前修改</p>
<p><strong>Staged:</strong> 暂存状态. 执行git commit则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为Unmodify状态. 执行git reset HEAD filename取消暂存,文件状态为Modified</p>
<p>下面的图很好的解释了这四种状态的转变：</p>
<p><img src="https://img2018.cnblogs.com/blog/1090617/201810/1090617-20181008212245877-52530897.png" alt></p>
<h1 id="四个区域之间的命令"><a href="#四个区域之间的命令" class="headerlink" title="四个区域之间的命令"></a>四个区域之间的命令</h1><h4 id="1、新建代码库"><a href="#1、新建代码库" class="headerlink" title="1、新建代码库"></a>1、新建代码库</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在当前目录新建一个Git代码库</span><br><span class="line">git init # 新建一个目录，将其初始化为Git代码库</span><br><span class="line">git init [project-name] # 下载一个项目和它的整个代码历史</span><br><span class="line">git clone [url]</span><br></pre></td></tr></table></figure>
<h4 id="2、查看文件状态"><a href="#2、查看文件状态" class="headerlink" title="2、查看文件状态"></a>2、查看文件状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#查看指定文件状态</span><br><span class="line">git status [filename] #查看所有文件状态</span><br><span class="line">git status</span><br></pre></td></tr></table></figure>
<h4 id="3、工作区-lt-—-gt-暂存区"><a href="#3、工作区-lt-—-gt-暂存区" class="headerlink" title="3、工作区&lt;—&gt;暂存区"></a>3、工作区&lt;—&gt;暂存区</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加指定文件到暂存区</span><br><span class="line">git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录</span><br><span class="line">git add [dir] # 添加当前目录的所有文件到暂存区</span><br><span class="line">git add . #当我们需要删除暂存区或分支上的文件, 同时工作区也不需要这个文件了, 可以使用（⚠️）</span><br><span class="line">git rm file_path #当我们需要删除暂存区或分支上的文件, 但本地又需要使用, 这个时候直接push那边这个文件就没有，如果push之前重新add那么还是会有。</span><br><span class="line">git rm --cached file_path #直接加文件名   从暂存区将文件恢复到工作区，如果工作区已经有该文件，则会选择覆盖 #加了【分支名】 +文件名  则表示从分支名为所写的分支名中拉取文件 并覆盖工作区里的文件</span><br><span class="line">git checkout</span><br></pre></td></tr></table></figure>
<h4 id="4、工作区-lt-—-gt-资源库（版本库）"><a href="#4、工作区-lt-—-gt-资源库（版本库）" class="headerlink" title="4、工作区&lt;—&gt;资源库（版本库）"></a>4、工作区&lt;—&gt;资源库（版本库）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#将暂存区--&gt;资源库（版本库）</span><br><span class="line">git commit -m &apos;该次提交说明&apos;</span><br><span class="line">#如果出现:将不必要的文件commit 或者 上次提交觉得是错的  或者 不想改变暂存区内容，只是想调整提交的信息</span><br><span class="line">#移除不必要的添加到暂存区的文件</span><br><span class="line">git reset HEAD 文件名 #去掉上一次的提交（会直接变成add之前状态） </span><br><span class="line">git reset HEAD^ </span><br><span class="line">#去掉上一次的提交（变成add之后，commit之前状态） </span><br><span class="line">git reset --soft  HEAD^</span><br></pre></td></tr></table></figure>
<h4 id="5、远程操作"><a href="#5、远程操作" class="headerlink" title="5、远程操作"></a>5、远程操作</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 取回远程仓库的变化，并与本地分支合并</span><br><span class="line">git pull # 上传本地指定分支到远程仓库</span><br><span class="line">git push</span><br></pre></td></tr></table></figure>
<h4 id="6、其它常用命令"><a href="#6、其它常用命令" class="headerlink" title="6、其它常用命令"></a>6、其它常用命令</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示当前的Git配置</span><br><span class="line">git config --list # 编辑Git配置文件</span><br><span class="line">git config -e [--global] #初次commit之前，需要配置用户邮箱及用户名，使用以下命令：</span><br><span class="line">git config --global user.email &quot;you@example.com&quot;</span><br><span class="line">git config --global user.name &quot;Your Name&quot;</span><br><span class="line">#调出Git的帮助文档</span><br><span class="line">git --help #查看某个具体命令的帮助文档</span><br><span class="line">git +命令 --help #查看git的版本</span><br><span class="line">git --version</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://www.cnblogs.com/qdhxhz/p/9757390.html" target="_blank" rel="noopener">https://www.cnblogs.com/qdhxhz/p/9757390.html</a></p>
]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>linux从入门到再入门</title>
    <url>/2020/linux%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%86%8D%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="Linux-工程必备"><a href="#Linux-工程必备" class="headerlink" title="Linux-工程必备"></a>Linux-工程必备</h1><p>一直以来，没有系统的学习过linux，每次遇到工程问题，总是靠着百度和谷歌，效率很低。<br>最近需要部署C++服务，所以需要LINUX环境，在此立帖，持续更新，直到熟练使用linux。</p><h1 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h1><a id="more"></a>
<ul>
<li>查看当前用户<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat /etc/passwd</span><br></pre></td></tr></table></figure></li>
<li>添加新用户<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ adduser 选项 username</span><br><span class="line">·</span><br><span class="line">选项可以有：</span><br><span class="line">-c 		comment 指定一段注释性描述。</span><br><span class="line">-d(常用）  目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。</span><br><span class="line">-g(常用）  用户组 指定用户所属的用户组。</span><br><span class="line">-G 		用户组，用户组 指定用户所属的附加组。</span><br><span class="line">-s 		Shell文件 指定用户的登录Shell。</span><br><span class="line">-u 		用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>例1：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd –d /usr/sam</span><br></pre></td></tr></table></figure><br>此命令创建了一个用户sam，其中-d选项用来为登录名sam产生一个主目录/usr/sam（/usr为默认的用户主目录所在的父目录）。</p>
<p>例2：<br>代码:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd -s /bin/sh -g group –G adm,root gem</span><br></pre></td></tr></table></figure><br>此命令新建了一个用户gem，该用户的登录Shell是/bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。</p>
<ul>
<li>给新用户加密码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ passwd 选项(不选最简单) 用户名</span><br><span class="line">·</span><br><span class="line">-l 锁定口令，即禁用账号。</span><br><span class="line">-u 口令解锁。</span><br><span class="line">-d 使账号无口令。</span><br><span class="line">-f 强迫用户下次登录时修改口令。</span><br><span class="line">如果默认用户名，则修改当前用户的口令。</span><br><span class="line">然后输入两次密码就行了。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>删除账号<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ userdel 选项(建议-r） 账户名</span><br><span class="line">-r 可以把主目录一起删除</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="文件权限与访问控制"><a href="#文件权限与访问控制" class="headerlink" title="文件权限与访问控制"></a>文件权限与访问控制</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200107231346.png" alt><br>Linux文件系统权限：</p>
<p><img src="https://img-blog.csdn.net/20180711160810276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQ0MjcxMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></p>
<h1 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h1><p><strong>一、Linux系统的结构</strong></p>
<p>　　1、Linux是一个倒树型结构，最大的目录名称为“/”（根目录）</p>
<p>　　2、Linux系统的二级目录</p>
<p>　　/bin　　 ##binary二进制可执行文件，系统常规命令<br>　　/boot 　　 ##启动目录，存放系统自动启动文件，内核，初始化程序<br>　　/dev ##系统设备管理文件<br>　　/etc　　 ##大多数系统配置文件存放路径<br>　　/home 　 ##普通用户家目录（/home/student）<br>　　/media 　 ##临时的挂载点<br>　　/lib　　 ##函数库<br>　　/lib64 　　 ##64位函数库（含有.bll）<br>　　/mnt 　　 ##临时挂载点<br>　　/run　　 ##自动临时设备挂载点（u盘）<br>　　/opt　　 ##第三方软件安装的位置<br>　　/sbin 　　 ##系统管理命令，通常只有root可以执行<br>　　/proc 　　 ##系统硬件信息和系统进程信息~~~~<br>　　/srv 　　 ##系统数据目录<br>　　/var 　　 ##系统数据目录<br>　　/sys　　 ##内核相关数据<br>　　/usr 　　　 ##用户相关信息数据<br>　　/tmp 　　 ##临时文件产生目录<br>　　/root　　 ##超级用户家目录</p>
<p>　　<strong><em>使用mount命令来更改临时设备的挂载点</em></strong></p>
<p><strong>二、文件管理命令</strong></p>
<p>　　1、文件的建立</p>
<p>　　命令：touch　filename　　## 通常用来创建文件，也可以修改文件的时间戳</p>
<p>　　注释：时间戳分为atime、mtime、ctime</p>
<p>　　　　atime　：文件内容被访问的时间标识</p>
<p>　　　　mtime　：文件内容被修改的时间标识</p>
<p>　　　　ctime　 ：文件属性或文件内容被修改的时间标识</p>
<p>　　实例：使用<strong><em> touch file </em></strong> 建立一个名为file的文件，并使用stat命令进行查看</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716191242211-1069722524.png" alt></p>
<ul>
<li>　若进行文件的查看后，则访问时间将会被改变，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716191629214-2046658304.png" alt></p>
<ul>
<li>　若文件进行编辑后，则访问时间、修改时间和文件改变时间三者均会变化，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716192012796-193555712.png" alt></p>
<p>　　注意：使用<strong><em> touch —help </em></strong>进行其他参数的查看</p>
<p>　　2、目录的建立</p>
<p>　　命令：mkdir　directory　　　　　　　　## 用来建立名为directory的目录</p>
<p>　　　　 mkdir　-p　test/redcat/linux　　 ## -p 进行多级目录的创建</p>
<p>　　注释：也可使用 mkdir —help命令进行参数的查看</p>
<p>　　实例：使用<strong><em> mkdir niu </em></strong>创建一个目录名为niu，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716192938835-574407116.png" alt></p>
<ul>
<li>　多级目录创建结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716193136101-657056986.png" alt></p>
<p>　　3、文件的删除</p>
<p>　　命令：rm 　file　　　　 ## 进行文件的删除</p>
<p>　　　　 rm 　-f 　test　　## -f 为强行删除文件</p>
<p>　　实例：使用<strong><em> ls file </em></strong>命令删除文件file，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716193719450-1436996021.png" alt></p>
<p>　　4、目录的删除</p>
<p>　　命令：rm 　-r 　directory　　## -r表示递归删除所有内容</p>
<p>　　　　 rm　 -r　-f 　dir　　　## 删除目录不再提示</p>
<p>　　　　 rm 　-rf　　dir　　　 ## j结果与上一个相同，且有 -a -b -c= - abc = - cba　</p>
<p>　　实例：使用<strong><em> rm -rf test</em></strong> 删除test目录以及test目录下的所有内容，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716194551519-1288801399.png" alt></p>
<p>　　5、文件编辑</p>
<ul>
<li>　gedit 编辑器</li>
</ul>
<p>　　　　　命令：gedit　　file　　## 必须有图形界面，进行file文件的编辑</p>
<ul>
<li>　vim 编辑器</li>
</ul>
<p>　　　　　命令：vim　file ———&gt; 按 i 进入insert 模式 ———&gt; 书写内容 ———&gt; esc退出insert模式 - ——-&gt; wq退出并保存</p>
<p>　　 实例：gedit使用（使用以下命令即可打开file文件，并进行编辑）</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716195422572-1655897153.png" alt></p>
<ul>
<li>　使用vim.tiny实例应用如下：（vim 和vim.tiny功能类似）</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716195752419-614443396.png" alt></p>
<p>　　###### 使用vim 会出现异常情况 ######</p>
<p>　　当vim异常退出时，会生成.file.swp文件（原因是修改文件未保存）</p>
<p>　　当helloc未保存后再次打开时，会出现以下情况：（下面文字接着图的more）<br>　 <img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716200340278-2049532639.png" alt></p>
<p>　　Swap file “.hello.swp” already exists!</p>
<p>　　[O]pen Read-Only, (E)dit anyway, (R)ecover, (D)elete it,　　 (Q)uit, (A)bort:<br>　　　　只读打开 　　　 继续编辑 　 恢复数据 　 删除swap文件 退出<br>　　分析：无论按【o】【e】【r】【q】【a】任何一个都不会删除.swp文件，再次打开还会</p>
<p>　　　　　有这样的这个问题，直到按【D】后，.swp被删除，vim恢复正常。  </p>
<p>　　6、文件的复制（复制目录的时候用- r）</p>
<p>　　命令：cp 　sourcefile　objectfile　　　　　　　　　## 表示把远文件复制到目标文件　</p>
<p>　　　　　cp 　-r　源目录　目的地目录　　</p>
<p>　　　　　cp 　源文件1　源文件2　目的地目录　　 　　## 目的地目录必须存在</p>
<p>　　　　　cp 　-r 　源目录1　源目录2　目的地目录　　 ## 目的地目录必须存在</p>
<p>　　实例：把file文件中的内容复制到file1中，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716202413095-1447577145.png" alt></p>
<ul>
<li>　使用 <strong><em> cp -r test　test1 </em></strong>命令把test目录下内容复制到test1目录中，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716203157814-1198844011.png" alt></p>
<ul>
<li>　使用 <strong><em> cp file1 file2 dir </em></strong>命令，把file1和file2文件复制到目录dir下，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716203804044-2015492922.png" alt></p>
<ul>
<li>　使用<strong><em> cp -r dir dir1 dir2 </em></strong>把目录dir1和dir2复制到目录dir3下，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716204213938-2053126740.png" alt></p>
<p>　　7、文件的移动</p>
<p>　　命令：mv 　源文件　　目的地文件　　　　## 重命名</p>
<p>　　　　　mv　 源目录　　目的地目录</p>
<p>　　实例：使用<strong><em> mv file file1</em></strong>命令，把file重命名为file1，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716204620780-1893321132.png" alt></p>
<p>　　使用<strong><em> mv niu/file test/ </em></strong>把niu目录下的file文件移动到test目录中，结果如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716204956300-1776343291.png" alt></p>
<p>　　（.代表当前目录）例：把test目录下的file1复制到当前目录下，命令如下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716205318027-1553458611.png" alt></p>
<p>　　注意：相同磁盘的文件移动是重命名的过程，不用磁盘的移动是复制删除的过程。</p>
<p>　　8、文件的查看</p>
<p>　　命令：cat　filename　　　　## 表示查看文件的全部内容</p>
<p>　　　　　cat 　-b 　filename　 ## 查看内容并显示行号</p>
<p>　　　　　less　filename　　　 ## 分页浏览(以下命令在less命令之后的操作)</p>
<p>　　　　　上|下　　　　　　　　## 逐行移动</p>
<p>　　　　　pageu|pagedn　　　 ## 逐页移动</p>
<p>　　　　　/关键字　　　　　　　## 高亮显示关键字，n向下匹配，N向上匹配</p>
<p>　　　　　v　　　　　　　　　　## 进入vim模式，然后按i进行编辑，返回vim模式按esc</p>
<p>　　　　　q　　　　　　　　　　##退出vim模式</p>
<p>　　实例：使用<strong><em> cat file1</em></strong>命令查看file1中的内容，结果入下：</p>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716212743634-353476679.png" alt></p>
<ul>
<li>　查看内容并显示行号，结果如下：</li>
</ul>
<p>　　<img src="https://images2018.cnblogs.com/blog/1438668/201807/1438668-20180716212922248-1273385407.png" alt></p>
<ul>
<li>　less 命令既修改文件中的内容，也可以使用快捷键进行查找，在此就不放截图了。</li>
</ul>
<p>　　9、文件的寻址</p>
<p>　　相对路径：</p>
<p>　　　　　　相对与当前系统所在的目录的一个文件名称的简写；</p>
<p>　　　　　　此名称省略了系统当前所在目录的名称；</p>
<p>　　　　　　此名称不以“/”开头；</p>
<p>　　　　　　此名称在命令执行时会自动在操作对象前加入”pwd”所显示的值。</p>
<p>　　绝对路径：</p>
<p>　　　　　　绝对路径是文件在系统中的真实位置；</p>
<p>　　　　　　此命令以“/”开头；</p>
<p>　　　　　　此命令在执行时不会考虑现在的位置的信息。</p>
<p>　　注意：当操作对象是　　对象1　空格　对象2 时，这两个对象没有任何关系</p>
<p>　　　　　亲　　　　## 动作时被系统执行，不能作为名称出现</p>
<p>　　　　　“亲”　　　 ## 引号的作用是把动作变成名称字符，这种方法叫引用</p>
<p>　　　　　pwd　　　## 显示当前工作目录</p>
<p>　　10、自动补齐</p>
<p>　　《tab》</p>
<p>　　　　　系统中的《tab》键可以实现命令的自动补齐；</p>
<p>　　　　　可以补齐系统中存在的命令，文件名称，和部分命令的参数；</p>
<p>　　　　　当一次《tab》补齐不了时，代表以此关键字开头的内容不唯一；</p>
<p>　　　　　可以使用《tab》两次来列出所有一次关键字开头的内容散</p>
<h1 id="脚本交互-except工具"><a href="#脚本交互-except工具" class="headerlink" title="脚本交互-except工具"></a>脚本交互-except工具</h1><p>主要用户自动操作，比如说：自动输入密码(最常见)<br>在使用expect时，基本上都是和以下四个命令打交道：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">send	用于向进程发送字符串</span><br><span class="line">expect	从进程接收字符串</span><br><span class="line">spawn	启动新的进程</span><br><span class="line">interact	允许用户交互</span><br></pre></td></tr></table></figure>
<p>send命令接收一个字符串参数，并将该参数发送到进程。</p>
<p>expect命令和send命令相反，expect通常用来等待一个进程的反馈，我们根据进程的反馈，再发送对应的交互命令。</p>
<p>spawn命令用来启动新的进程，spawn后的send和expect命令都是和使用spawn打开的进程进行交互。</p>
<p>interact命令用的其实不是很多，一般情况下使用spawn、send和expect命令就可以很好的完成我们的任务；但在一些特殊场合下还是需要使用interact命令的，interact命令主要用于退出自动化，进入人工交互。</p>
<ul>
<li>例子：链接shh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/tcl/bin/expect</span><br><span class="line"></span><br><span class="line">set timeout 30</span><br><span class="line">set host &quot;101.200.241.109&quot;</span><br><span class="line">set username &quot;root&quot;</span><br><span class="line">set password &quot;123456&quot;</span><br><span class="line"></span><br><span class="line">spawn ssh $username@$host</span><br><span class="line">expect &quot;*password*&quot; &#123;send &quot;$password\r&quot;&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
<ul>
<li>例子：自动切换到sudo 并启动hexo<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/expect</span><br><span class="line">set timeout 30</span><br><span class="line">spawn sudo -s</span><br><span class="line">expect &quot;Password:&quot;</span><br><span class="line">send &quot;123456\r&quot;</span><br><span class="line">expect &quot;*#&quot;</span><br><span class="line">send &quot;cd hexoblog\r&quot;</span><br><span class="line">expect &quot;*#&quot;</span><br><span class="line">send &quot;hexo s\r&quot;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>参考：<a href="https://blog.csdn.net/weixin_42442713/article/details/81001753" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42442713/article/details/81001753</a><br><a href="https://www.cnblogs.com/uthnb/p/9320582.html" target="_blank" rel="noopener">https://www.cnblogs.com/uthnb/p/9320582.html</a></p>
]]></content>
      <categories>
        <category>工程</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>5003总结3 Graphs</title>
    <url>/2020/5003%E6%80%BB%E7%BB%933-Graphs/</url>
    <content><![CDATA[<p>graphframes是处理图的一个API <a href="https://graphframes.github.io/graphframes/docs/_site/index.html" target="_blank" rel="noopener">graphframes官方文档</a><br>可以和RDD结合，和pyspark结合使用。<br>在Python环境使用需要以下步骤：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**GraphFrames:**</span><br><span class="line">-   For pre-installed Spark version ubuntu, to use GraphFrames:</span><br><span class="line">    1.  get the jar file:  </span><br><span class="line">        wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.7.0-spark2.4-s_2.11/graphframes-0.7.0-spark2.4-s_2.11.jar</span><br><span class="line">    2.  Load the jar file in the Jupyter notebook  </span><br><span class="line">        sc.addPyFile(&apos;path_to_the_jar_file&apos;)</span><br><span class="line">    You can also refer to &quot;~/Untitled.ipynb&quot;.</span><br><span class="line">-   Using the pyspark shell directly with GraphFrames:</span><br><span class="line">    -   ./bin/pyspark --packages graphframes:graphframes:0.7.0-spark2.4-s_2.11</span><br><span class="line">-   Using Jupyter locally:</span><br><span class="line">    1.  Set the environment variable:  </span><br><span class="line">        export SPARK_OPTS=&quot;--packages graphframes:graphframes:0.7.0-spark2.4-s_2.11&quot;</span><br><span class="line">    2.  get the jar file:  </span><br><span class="line">        wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.7.0-spark2.4-s_2.11/graphframes-0.7.0-spark2.4-s_2.11.jar</span><br><span class="line">    3.  Load the jar file in the Jupyter notebook  </span><br><span class="line">        sc.addPyFile(&apos;path_to_the_jar_file&apos;)</span><br><span class="line">-   In Azure Databricks Service:</span><br><span class="line">    1.  Start the cluster</span><br><span class="line">    2.  Search for &quot;graphframes&apos; and install the library</span><br></pre></td></tr></table></figure></p><a id="more"></a>
<p>在jupyter里面使用需要引用压缩包，例如：<br><code>sc.addPyFile(&quot;/usr/local/Cellar/apache-spark/2.4.4/libexec/jars/graphframes-0.7.0-spark2.4-s_2.11.jar&quot;)</code></p>
<hr>
<h1 id="生成图"><a href="#生成图" class="headerlink" title="生成图"></a>生成图</h1><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">v = spark.createDataFrame([</span><br><span class="line">  (<span class="string">"a"</span>, <span class="string">"Alice"</span>, <span class="number">34</span>),</span><br><span class="line">  (<span class="string">"b"</span>, <span class="string">"Bob"</span>, <span class="number">36</span>),</span><br><span class="line">  (<span class="string">"c"</span>, <span class="string">"Charlie"</span>, <span class="number">37</span>),</span><br><span class="line">  (<span class="string">"d"</span>, <span class="string">"David"</span>, <span class="number">29</span>),</span><br><span class="line">  (<span class="string">"e"</span>, <span class="string">"Esther"</span>, <span class="number">32</span>),</span><br><span class="line">  (<span class="string">"f"</span>, <span class="string">"Fanny"</span>, <span class="number">38</span>),</span><br><span class="line">  (<span class="string">"g"</span>, <span class="string">"Gabby"</span>, <span class="number">60</span>)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>])</span><br><span class="line"><span class="comment"># Edges DataFrame</span></span><br><span class="line">e = spark.createDataFrame([</span><br><span class="line">  (<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"friend"</span>),</span><br><span class="line">  (<span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"follow"</span>),</span><br><span class="line">  (<span class="string">"c"</span>, <span class="string">"b"</span>, <span class="string">"follow"</span>),</span><br><span class="line">  (<span class="string">"f"</span>, <span class="string">"c"</span>, <span class="string">"follow"</span>),</span><br><span class="line">  (<span class="string">"e"</span>, <span class="string">"f"</span>, <span class="string">"follow"</span>),</span><br><span class="line">  (<span class="string">"e"</span>, <span class="string">"d"</span>, <span class="string">"friend"</span>),</span><br><span class="line">  (<span class="string">"d"</span>, <span class="string">"a"</span>, <span class="string">"friend"</span>),</span><br><span class="line">  (<span class="string">"a"</span>, <span class="string">"e"</span>, <span class="string">"friend"</span>),</span><br><span class="line">  (<span class="string">"g"</span>, <span class="string">"e"</span>, <span class="string">"follow"</span>)</span><br><span class="line">], [<span class="string">"src"</span>, <span class="string">"dst"</span>, <span class="string">"relationship"</span>])</span><br><span class="line"><span class="comment"># Create a GraphFrame</span></span><br><span class="line">g = GraphFrame(v, e)</span><br><span class="line"></span><br><span class="line">g.vertices.show()</span><br><span class="line">g.edges.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123161350.png" alt><br>建立两个dataframe：</p>
<ul>
<li>一个储存节点vertices的id+[data]</li>
<li>另一个储存关系edges的[“src”, “dst”, “relationship”]</li>
</ul>
<p>这个图就建立好了。</p>
<h1 id="图的基本属性"><a href="#图的基本属性" class="headerlink" title="图的基本属性"></a>图的基本属性</h1><p>g.vertices和g.edges是输入的两个v，e 可以使用dataframe的方法筛选过滤统计。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123140805.png" alt></p>
<p>g.outDegrees或者g.inDegrees可以返回一个dataframe-[id,degree]，自动计算每个节点的出度和入度。</p>
<ul>
<li>cache也可用到图上<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123141021.png" alt><br>整个图cache 运用：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123144609.png" alt><h1 id="Motif-finding"><a href="#Motif-finding" class="headerlink" title="Motif finding"></a>Motif finding</h1></li>
</ul>
<p>Motif finding refers to searching for structural patterns in a graph.<br>例如：寻找互相关注，寻找三角形关注<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123145700.png" alt><br>寻找单向关注，寻找无人关注<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123145801.png" alt></p>
<p>另一种方法寻找无人关注的点：</p>
<ul>
<li><p>左连接or求差集（先求id的差集，再直接join整张表）<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123150855.png" alt></p>
<blockquote>
<p>如果不是左连接，那一行会被删掉</p>
</blockquote>
</li>
<li><p>例子：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123151645.png" alt></p>
</li>
</ul>
<p>先找单向的四人关系，map 相互为friend=1，筛选有2个friend以上的关系链。</p>
<h1 id="Subgraphs-生成子图"><a href="#Subgraphs-生成子图" class="headerlink" title="Subgraphs 生成子图"></a>Subgraphs 生成子图</h1><ul>
<li>选择子节点和子边</li>
<li><p>生成新图<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123152823.png" alt></p>
</li>
<li><p>过滤掉多余的部分<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123153344.png" alt><br>这里的leftsemi的join方式：<a href="https://blog.csdn.net/zhaoxz128/article/details/80784188" target="_blank" rel="noopener">leftsemi讲解</a></p>
<blockquote>
<p>Hive 当前<strong>没有</strong>实现 IN/EXISTS 子查询，所以你可以用 <strong>LEFT SEMI JOIN 重写你的子查询语句</strong>。LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在  ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>
<h1 id="例子：找到最少被两个人关注的人"><a href="#例子：找到最少被两个人关注的人" class="headerlink" title="例子：找到最少被两个人关注的人"></a>例子：找到最少被两个人关注的人</h1><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">g.find(<span class="string">"( )-[e]-&gt;(b)"</span>).filter(<span class="string">"e.relationship='follow'"</span>).</span><br><span class="line">groupby(<span class="string">'b'</span>).count().filter(<span class="string">"count&gt;=2"</span>).select(<span class="string">'b.name'</span>).show()</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
<h1 id="应用：BFS广度有限搜索"><a href="#应用：BFS广度有限搜索" class="headerlink" title="应用：BFS广度有限搜索"></a>应用：BFS广度有限搜索</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123161040.png" alt><br>API自带的接口：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123161117.png" alt></p>
<h1 id="例子-list-rank"><a href="#例子-list-rank" class="headerlink" title="例子 list rank"></a>例子 list rank</h1><p>对于一个链表排序（按照距离尾部节点的距离排序）：<br>暴力算法：遍历n次，每次找到尾部节点，记录并删除 O(n)<br>方法2：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Initialize u.d = 0 if u.next = null, else u.d = 1</span><br><span class="line">Run the following for each node u until all next pointers are null:</span><br><span class="line">	if u.next is not null: </span><br><span class="line">		u.d += u.next.d </span><br><span class="line">		u.next = u.next.next</span><br></pre></td></tr></table></figure><br>算法原理如图：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123170545.png" alt><br>复杂度O(logn)，每一轮遍历非尾节点都会参与，减少循环次数<br> 实现：<br> <img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123170704.png" alt><br> <img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123170718.png" alt></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文总结了pyspark的graph模块基本语法和操作，关于5003的三篇总结到此结束。</p>
]]></content>
      <tags>
        <tag>复习</tag>
        <tag>spark</tag>
        <tag>5003</tag>
        <tag>并行计算</tag>
      </tags>
  </entry>
  <entry>
    <title>5003总结2 stream&amp;并行计算-例子</title>
    <url>/2019/5003%E6%80%BB%E7%BB%932-stream-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-%E4%BE%8B%E5%AD%90-1/</url>
    <content><![CDATA[<p>Streaming是处理数据流的API。<br>下面直接通过例子，感受streaming的机制。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123213735.png" alt=""><br><a id="more"></a></p>
<h1 id="例1-从键盘输入文本，统计词频"><a href="#例1-从键盘输入文本，统计词频" class="headerlink" title="例1 从键盘输入文本，统计词频"></a>例1 从键盘输入文本，统计词频</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123220112.png" alt=""></p>
<h1 id="例2-1-从文件输入，统计词频"><a href="#例2-1-从文件输入，统计词频" class="headerlink" title="例2.1 从文件输入，统计词频"></a>例2.1 从文件输入，统计词频</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123222512.png" alt=""></p>
<h1 id="例2-2-对词的感情值排序"><a href="#例2-2-对词的感情值排序" class="headerlink" title="例2.2 对词的感情值排序"></a>例2.2 对词的感情值排序</h1><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123230557.png" alt=""></p>
<h1 id="例2-3-stage，统计所有词频"><a href="#例2-3-stage，统计所有词频" class="headerlink" title="例2.3 stage，统计所有词频"></a>例2.3 stage，统计所有词频</h1><p>上面的例子2.1 仅仅是统计每一个输入部分的词频，意义不大<br>通过<code>updateStateByKey()</code>可以实现对所有数据的统计。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191123231336.png" alt=""></p>
<p><code>updateStateByKey()</code>需要定义一个函数，函数的输入是上次的和这次的rdd，这里的streaming的精髓，需要好好理解。</p>
<hr>
<h1 id="算法应用1：蓄水池抽样Reservoir-Sampling"><a href="#算法应用1：蓄水池抽样Reservoir-Sampling" class="headerlink" title="算法应用1：蓄水池抽样Reservoir Sampling"></a>算法应用1：蓄水池抽样Reservoir Sampling</h1><h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>蓄水池采样算法（Reservoir Sampling）了。先说一下算法的过程：</p>
<ul>
<li>假设数据序列的规模为  𝑛，需要采样的数量的为  𝑘。</li>
<li>首先构建一个可容纳  𝑘  个元素的数组，将序列的前  𝑘  个元素放入数组中。</li>
<li>然后从第  𝑘+1  个元素开始，以  𝑘/𝑛  的概率来决定该元素是否被替换到数组中（数组中的元素被替换的概率是相同的）。 当遍历完所有元素之后，数组中剩下的元素即为所需采取的样本。<blockquote>
<p>这样就可以对任意长度的数据抽样，可以实现streaming. 不需要考虑文件长度。</p>
</blockquote>
</li>
</ul>
<p>简单证明：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191124085220.png" alt=""></p>
<p>完整证明需要用到FIsher-Yates shuffle。</p>
<h1 id="算法应用2：O-n-时间，O-1-空间，寻找Majority的元素-n-gt-N-2"><a href="#算法应用2：O-n-时间，O-1-空间，寻找Majority的元素-n-gt-N-2" class="headerlink" title="算法应用2：O(n)时间，O(1)空间，寻找Majority的元素(n&gt;N/2)"></a>算法应用2：O(n)时间，O(1)空间，寻找Majority的元素(n&gt;N/2)</h1><p>算法描述：<br>只用空间复杂度O(1)，去寻找数组内有没有出现次数大于一半的元素。</p>
<p>算法过程：</p>
<blockquote>
<p>遍历所有元素：记录新出现元素的次数，如果下一个与记录相同，就＋1，不同就-1，遍历完成，会保留一个候选项<br>遍历所有元素：统计刚才候选项出现的次数n以及N，验证n&gt;N/2则验证成功，如果n&lt;=N/2，则无Majority</p>
</blockquote>
<h1 id="算法应用3：GM算法-Heavy-hitters"><a href="#算法应用3：GM算法-Heavy-hitters" class="headerlink" title="算法应用3：GM算法 Heavy hitters"></a>算法应用3：GM算法 Heavy hitters</h1><p>Misra-Gries (MG) algorithm finds up to k items that occur more than 1/k fraction of the time in a stream.<br>找到频率超过1/k的元素.<br>算法：</p>
<blockquote>
<p>初始化k个候选项，对之后的每个元素：</p>
<ul>
<li>如果元素已经被记录了，count++</li>
<li>如果没有记录，如果候选项&lt;k，那就记录他</li>
<li>如果没有记录，如果候选项=k，那就对每个候选项count—</li>
</ul>
<p>遍历完成，返回候选项</p>
</blockquote>
<h1 id="算法应用4：并行计算TOPn-时间复杂度满足O-n-p-log-k"><a href="#算法应用4：并行计算TOPn-时间复杂度满足O-n-p-log-k" class="headerlink" title="算法应用4：并行计算TOPn, 时间复杂度满足O(n/p*log k)"></a>算法应用4：并行计算TOPn, 时间复杂度满足O(n/p*log k)</h1><p>原题：</p>
<blockquote>
<p>Given an RDD storing a list of n integers (unordered), design a divide-and-conquer algorithm to find the k largest integers in the RDD. All workers must run in parallel. For full marks, each worker should run in time O(n/p * log k) time, where p is the number of workers.<br>Hint: Use a priority queue at each worker for maximum efficiency.</p>
</blockquote>
<p>当看到log K和priority queue就知道要使用堆排序了。<br>分成p个worker，每个worker有O(n/p)的值，每一个work维护一个最大K堆，时间复杂度是O(log k). </p>
<p>老师给的答案：</p>
<blockquote>
<p>Partition the integers evenly so that each worker receives O(n/p) values. On each worker, maintain a min-heap of size k. Inserting an element to the heap costs O(log k) time.<br>For each value, insert it to the heap if it is larger than the current min of the heap. Pop the minimum item if there are more than k items in the heap. By making one pass over all the values in each worker, we get the k largest integers out of the values of this worker. This can be done in O(n/p log k) time.<br>Then we merge the results by sending the top k of each partition to the same worker, which is at most kp values. The worker finds the largest k integers among these kp values and report it.</p>
</blockquote>
<p>代码<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq  <span class="comment"># 引入最大堆的包</span></span><br><span class="line">  </span><br><span class="line">rdd = sc.parallelize(xrange(<span class="number">0</span>,<span class="number">1000</span>,<span class="number">2</span>))   <span class="comment">#生成rdd</span></span><br><span class="line">k = <span class="number">10</span>  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topk</span><span class="params">(it)</span>:</span>        <span class="comment">#对于每一个分区的n/p个值</span></span><br><span class="line">    h = []  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> it:     <span class="comment">#遍历每一个值</span></span><br><span class="line">        <span class="comment"># check if we should insert i  </span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> h) <span class="keyword">or</span> (i &gt; h[<span class="number">0</span>]):   <span class="comment">#如果h为空或者新来的大于h里面最大的</span></span><br><span class="line">            heapq.heappush(h,i)     <span class="comment">#把这个元素插入到h</span></span><br><span class="line">            <span class="keyword">if</span> len(h) &gt; k:          <span class="comment">#长度大于k</span></span><br><span class="line">                heapq.heappop(h)    <span class="comment">#弹出h中最小的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(h)):        <span class="comment">#遍历每一个值</span></span><br><span class="line">        <span class="keyword">yield</span> heapq.heappop(h)      <span class="comment">#从小到大依次弹出返回到上一层</span></span><br><span class="line">        </span><br><span class="line">print(list(topk(rdd.mapPartitions(topk).collect()))) <span class="comment">#对于分区使用，再整体使用</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p>用实例总结了几个steaming和并行计算的基础知识点，路漫漫兮。</p>
]]></content>
      <tags>
        <tag>复习</tag>
        <tag>spark</tag>
        <tag>5003</tag>
        <tag>并行计算</tag>
      </tags>
  </entry>
  <entry>
    <title>5003总结1 RDD, Spark Internals</title>
    <url>/2019/5003%E6%80%BB%E7%BB%931-RDD-Spark-Internals/</url>
    <content><![CDATA[<h1 id="5003总结1-RDD-Spark-Internals"><a href="#5003总结1-RDD-Spark-Internals" class="headerlink" title="5003总结1 RDD, Spark Internals"></a>5003总结1 RDD, Spark Internals</h1><p>学习了5003之后，对于RDD和spark两章总结复习，梳理出有关函数的作用。</p><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>resilient Distributed Datasets: 弹性分布式数据库，rdd是spark的基本运行单位<br>使用者作用于RDD，RDD自动进行分区，在不同partitions进行操作<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030092656.png" alt="RDD的操作"></p><a id="more"></a>

<h2 id="rdd是“懒惰的”Lazy，如果只进行Transfomations-，RDD不会进行实际运算，会等待actions操作才会实际运算。（理解起来就是，如果每Transfomations一次就要运算，就需要操作一下等一次；而且Transfomations之间可能有优化的空间；所以如果没有Actions就意味着不需要输出，就先记住步骤不执行）"><a href="#rdd是“懒惰的”Lazy，如果只进行Transfomations-，RDD不会进行实际运算，会等待actions操作才会实际运算。（理解起来就是，如果每Transfomations一次就要运算，就需要操作一下等一次；而且Transfomations之间可能有优化的空间；所以如果没有Actions就意味着不需要输出，就先记住步骤不执行）" class="headerlink" title="* rdd是“懒惰的”Lazy，如果只进行Transfomations ，RDD不会进行实际运算，会等待actions操作才会实际运算。（理解起来就是，如果每Transfomations一次就要运算，就需要操作一下等一次；而且Transfomations之间可能有优化的空间；所以如果没有Actions就意味着不需要输出，就先记住步骤不执行）"></a>* rdd是“懒惰的”Lazy，如果只进行Transfomations ，RDD不会进行实际运算，会等待actions操作才会实际运算。（理解起来就是，如果每Transfomations一次就要运算，就需要操作一下等一次；而且Transfomations之间可能有优化的空间；所以如果没有Actions就意味着不需要输出，就先记住步骤不执行）</h2><p><a href="https://haofly.net/spark/" target="_blank" rel="noopener">引用博文 https://haofly.net/spark/</a></p>
<ul>
<li><strong>RDD(Resilient Distributed Dataset弹性分布式数据集)</strong>：这是spark的主要数据概念。有多种来源，容错机制，并且能缓存、并行计算。RDD在整个计算流程中会经过不同方式的变换，这种变换关系就是一个有向无环图。</li>
<li>需要注意的是，所有的方法在定义执行之前都是异步的，所以不能简单地在下面的方法外部添加<code>try...catch...</code>进行异常捕获，最好是在传入的函数里面进行异常的捕获(如果是lambda，请确认lambda不会报错，否则如果lambda报错整个程序都会报错并终止允许)</li>
<li>Spark应用程序可以使用大多数主流语言编写，这里使用的是python，只<code>pip install pyspark</code>即可</li>
<li><strong>Stage(调度阶段)</strong>: 每个Job会根据RDD大小切分城多个Stage，每个Stage包含一个TaskSet</li>
<li><strong>TaskSet(任务集)</strong>: 一组关联的Task集合，不过是没有依赖的</li>
<li><strong>Task(任务)</strong>: RDD中的一个分区对应一个Task。</li>
<li><strong>Narrow Dependency(窄依赖)</strong>: 比较简单的一对一依赖和多对一依赖(如union)</li>
<li><strong>Shuffle Dependency(宽依赖)</strong>: 父RDD的分区被多个子RDD分区所使用，这时父RDD的数据会被再次分割发送给子RDD</li>
<li><strong>Spark 内存分配</strong>: 分为这三块:<ul>
<li><strong>execution</strong>: 执行内存，基本的算子都是在这里面执行的，这块内存满了就写入磁盘。</li>
<li><strong>storage</strong>: 用于存储broadcast, cache, persist</li>
<li><strong>other</strong>: 程序预留给自己的内存，这个可以不用考虑</li>
</ul>
</li>
<li><strong>Duration</strong><ul>
<li><strong>batchDuration</strong>: 批次时间</li>
<li><strong>windowDuration</strong>: 窗口时间，要统计多长时间内的数据，必须是<code>batchDuration</code>的整数倍</li>
<li><strong>slideDuration</strong>: 滑动时间，窗口多长时间滑动一次，必须是<code>batchDuration</code>的整数倍，一般是跟<code>batchDuration</code>时间相同<h3 id="生成RDD"><a href="#生成RDD" class="headerlink" title="生成RDD"></a>生成RDD</h3>sc.parallelize(xrange(10))<br>sc.textFile(‘../data/fruits.txt’)</li>
</ul>
</li>
</ul>
<h3 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a><a href="https://haofly.net/spark/#%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97" target="_blank" rel="noopener" title="基本运算"></a>基本运算</h3><ul>
<li>下面是所有运算方法的集合，其中有些方法仅用于键值对，有些方法仅用于数据流</li>
</ul>
<h4 id="Transformation-转换"><a href="#Transformation-转换" class="headerlink" title="Transformation(转换)"></a><a href="https://haofly.net/spark/#Transformation-%E8%BD%AC%E6%8D%A2" target="_blank" rel="noopener" title="Transformation(转换)"></a>Transformation(转换)</h4><p>这类方法仅仅是定义逻辑，并不会立即执行，即lazy特性。目的是将一个RDD转为新的RDD。</p>
<ul>
<li>map(func): 返回一个新的RDD，func会作用于每个map的key，func的返回值即是新的数据。为了便于后面的计算，这一步一般在数据处理的最前面将数据转换为(K, V)的形式，例如计数的过程中首先要<code>datas.map(lambda a, (a, 1))</code>将数据转换成(a, 1)的形式以便后面累加</li>
<li><p>flatmap： 提取出每个list的所有元素，压成一层</p>
</li>
<li><p>mappartitions(func, partition): 和map不同的地方在于map的func应用于每个元素，而这里的<strong>func会应用于每个分区</strong>，能够有效减少调用开销，减少func初始化次数。减少了初始化的内存开销。但是map如果数据量过大，计算后面的时候可以将已经计算过的内存销毁掉，但是mappartitions中如果一个分区太大，一次计算的话可能直接导致内存溢出。</p>
</li>
<li>filter(func): 返回一个新的RDD，func会作用于每个map的key，返回的仅仅是返回True的数据组成的集合，返回None或者False或者不返回都表示被过滤掉</li>
<li>filtMap(func): 返回一个新的RDD，func可以一次返回多个元素，最后形成的是所有返回的元素组成的新的数据集</li>
<li>mapValues(func): 返回一个新的RDD，对RDD中的每一个value应用函数func。</li>
<li>distinct(): 去除重复的元素</li>
<li>subtractByKey(other): 删除在RDD1中的RDD2中key相同的值</li>
<li>groupByKey(numPartitions=None): 将(K, V)数据集上所有Key相同的数据聚合到一起，得到的结果是(K, (V1, V2…))</li>
<li>reduceByKey(func, numPartitions=None): 将(K, V)数据集上所有Key相同的数据聚合到一起，func的参数即是每两个K-V中的V。可以使用这个函数来进行计数，例如reduceByKey(lambda a,b:a+b)就是将key相同数据的Value进行相加。</li>
<li>reduceByKeyAndWindow(func, invFunc, windowdurartion, slideDuration=None, numPartitions=None, filterFunc=None): 与reduceByKey类似，不过它是在一个时间窗口上进行计算，由于时间窗口的移动，有增加也有减少，所以必须提供一个逻辑和func相反的函数invFunc，例如func为(lambda a, b: a+b)，那么invFunc一般为(lambda a, b: a-b)，其中a和b都是key相同的元素的value。另外需要注意的是，程序默认会缓存一个时间窗口内所有的数据以便后续能进行inv操作，所以如果窗口太长，内存占用可能会非常高</li>
<li>join(other, numPartitions=None): 将(K, V)和(K, W)类型的数据进行类似于SQL的JOIN操作，得到的结果是这样(K, (V, W))</li>
<li>union(other): 并集运算，简单合并两个RDD</li>
<li>intersection(other): 交集运算，保留在两个RDD中都有的元素</li>
<li>leftOuterJoin(other): 左外连接</li>
<li>rightOuterJoin(other): 右外连接</li>
</ul>
<h4 id="Action-执行"><a href="#Action-执行" class="headerlink" title="Action(执行)"></a><a href="https://haofly.net/spark/#Action-%E6%89%A7%E8%A1%8C" target="_blank" rel="noopener" title="Action(执行)"></a>Action(执行)</h4><p>不会产生新的RDD，而是直接运行，得到我们想要的结果。</p>
<ul>
<li>collect(): 以数组的形式，返回数据集中所有的元素</li>
<li>count(): 返回数据集中元素的个数</li>
<li>take(n): 返回数据集的前N个元素</li>
<li>takeOrdered(n): 升序排列，取出前N个元素</li>
<li>takeOrdered(n, lambda x: -x): 降序排列，取出前N个元素</li>
<li>first(): 返回数据集的第一个元素</li>
<li>min(): 取出最小值</li>
<li>max(): 取出最大值</li>
<li>stdev(): 计算标准差</li>
<li>sum(): 求和</li>
<li>mean(): 平均值</li>
<li>countByKey(): 统计各个key值对应的数据的条数</li>
<li>lookup(key): 根据传入的key值来查找对应的Value值</li>
<li>foreach(func): 对集合中每个元素应用func</li>
</ul>
<h4 id="Persistence-持久化"><a href="#Persistence-持久化" class="headerlink" title="Persistence(持久化)"></a><a href="https://haofly.net/spark/#Persistence-%E6%8C%81%E4%B9%85%E5%8C%96" target="_blank" rel="noopener" title="Persistence(持久化)"></a>Persistence(持久化)</h4><ul>
<li>cache()：保存，固定</li>
<li>persist(): 将数据按默认的方式进行持久化</li>
<li>unpersist(): 取消持久化</li>
<li>saveAsTextFile(path): 将数据集保存至文件</li>
</ul>
<hr>
<h2 id="RDD的操作-pyspark"><a href="#RDD的操作-pyspark" class="headerlink" title="RDD的操作 pyspark"></a>RDD的操作 pyspark</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">fruits = sc.textFile(<span class="string">'../data/fruits.txt'</span>) </span><br><span class="line">fruits.collect()</span><br><span class="line"><span class="comment">#读文本，全部显示</span></span><br></pre></td></tr></table></figure>
<p>累加器的使用：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030104539.png" alt></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize(xrange(<span class="number">10</span>))</span><br><span class="line">accum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> accum</span><br><span class="line">    accum += x</span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line">a = rdd.map(g)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"---------"</span></span><br><span class="line"><span class="keyword">print</span> accum.value</span><br><span class="line"><span class="keyword">print</span> rdd.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"---------"</span></span><br><span class="line"><span class="comment">#a.cache()</span></span><br><span class="line">tmp = a.count()</span><br><span class="line"><span class="keyword">print</span> accum.value</span><br><span class="line"><span class="keyword">print</span> rdd.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"---------"</span></span><br><span class="line">tmp = a.count()</span><br><span class="line"><span class="keyword">print</span> accum.value</span><br><span class="line"><span class="keyword">print</span> rdd.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="comment">#reduce 始终不变，但是如果没有cache accumulator就会反复累加</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">---------</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">45</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">45</span></span><br><span class="line"><span class="number">45</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">90</span></span><br><span class="line"><span class="number">45</span></span><br></pre></td></tr></table></figure>
<p>展示分区glom（）<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030105120.png" alt><br>mapParttitions 在分区内部执行函数f<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030110438.png" alt></p>
<p>这里的index就是为了改变每次生成的随机数不一样，否则每个分区算出来是一样的<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030110351.png" alt></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030111556.png" alt><br>对key相同的值进行操作</p>
<h4 id="kmeans"><a href="#kmeans" class="headerlink" title="kmeans"></a>kmeans</h4><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030113309.png" alt></p>
<h4 id="pagerank"><a href="#pagerank" class="headerlink" title="pagerank"></a>pagerank</h4><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191031205443.png" alt></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191030113538.png" alt></p>
<h1 id="Internal-of-Spark"><a href="#Internal-of-Spark" class="headerlink" title="Internal of Spark"></a>Internal of Spark</h1><h2 id="web端监控Spark运行情况"><a href="#web端监控Spark运行情况" class="headerlink" title="web端监控Spark运行情况"></a>web端监控Spark运行情况</h2><p>查看Spark 可视化进程： localhost:4040 4041 4042 …</p>
<h2 id="partitions-分区"><a href="#partitions-分区" class="headerlink" title="partitions 分区"></a>partitions 分区</h2><p>RDD是储存在不同的partition里的，生成时每个partition平衡的（数量差不多），用于并行计算，但是有可能操作之后，就不平衡了。</p>
<p>这时候，需要 .repartition(n)</p>
<h1 id="Hash-vs-Range-partitioning"><a href="#Hash-vs-Range-partitioning" class="headerlink" title="Hash  vs Range partitioning"></a>Hash  vs Range partitioning</h1><ul>
<li>Hash partitioning<br>通过%N，这样的将相同<strong>余数</strong>放到一个分区。<br>缺点：可能由于原数据余数不平衡，可能分区不平衡</li>
<li>Range partitioning<br>计算每个分区大小，将连续的数放到一个分区<br>缺点：数值大小不平衡</li>
</ul>
<p>例子如下：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191031200235.png" alt></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191031201853.png" alt><br>RDD本来没有partitions，当有了shuffle 或者主动生成一个partitions才会有。<br>(这样的好处是，shuffle本身是需要多个partitions一起参与的，如果是线性图（key没有变化），那就只需要在自己的分区内计算，实现并行)</p>
<p>会继承partitions的3个Operations(key不会改变):</p>
<ul>
<li>mapValues  （但是map就不行）<ul>
<li>flatMapValues </li>
<li>filter  </li>
</ul>
</li>
</ul>
<p>其他的Operations都会改变key？ （这里需要考察一下）<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191031202823.png" alt></p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191031211821.png" alt></p>
<p>Spark 的 各级目录</p>
<blockquote>
<p>application 一个内核&gt;job一次运行 &gt;stage指令下的一个状态&gt;task一个状态的任务</p>
</blockquote>
<p>Spark的内存管理</p>
<ul>
<li>Two types of memory usages for applications:<br>– Execution memory: for computation in shuffles, joins, sorts and aggregations<br>– Storage memory: for caching and propagating internal data across the cluster</li>
</ul>
]]></content>
      <tags>
        <tag>复习</tag>
        <tag>spark</tag>
        <tag>5003</tag>
      </tags>
  </entry>
  <entry>
    <title>准备Q音推荐实习面试的笔记</title>
    <url>/2019/%E5%87%86%E5%A4%87Q%E9%9F%B3%E6%8E%A8%E8%8D%90%E7%9A%84%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>11月30更新，面试通过。后续更新实习心得。</p><hr><p>学姐有发消息招腾讯音乐的日常实习，我当天投递简历，当晚组长联系第二天面试。<br>推荐部门，我约的第二天下午三点面试，感觉下午会清醒一点。<br>现在记录一下准备过程。<br>本文没有逻辑，看到哪记到哪。</p><a id="more"></a>


<hr>
<h1 id="qq音乐推荐现状"><a href="#qq音乐推荐现状" class="headerlink" title="qq音乐推荐现状"></a>qq音乐推荐现状</h1><p>用户登录之后，默认进入喜好选择页面。<br>Page1：选择喜好的音乐流派，可多选，候选项为常见的流派<br>Page2：选择喜好的歌手，候选项为基于流派里的出名歌星<br>然后在推荐页，有电台，当天推荐30首，每周新歌推荐；<br>下面可以一直往下拉，有专辑和短视频推荐，短视频自动播放</p>
<p>喜好选择可以解决能启动。<br>解决冷启动还能通过：</p>
<blockquote>
<p>导入用户在社交网络上的好友信息和公开发布的信息<br>基本信息：（性别、职业、年龄段、地理位置（方言，城市等级，天气）、手机型号，传感器：行为判断运动？散步？休息）<br>关系链：相似好友推荐<br>朋友圈、QQ空间获取初步的用户画像：</p>
<ul>
<li>自己：个性签名，朋友圈动态-nlp，cv情感分析</li>
<li>交互：好友分享，点赞，评论</li>
</ul>
</blockquote>
<hr>
<h1 id="相似度矩阵思考问题"><a href="#相似度矩阵思考问题" class="headerlink" title="相似度矩阵思考问题"></a>相似度矩阵思考问题</h1><p>链接：<a href="https://link.jianshu.com/?t=https://www.zhihu.com/question/26743347/answer/34542247" target="_blank" rel="noopener">知乎沙克</a>  </p>
<p>完整的推荐系统体系包括 官方团队推荐（Editorial）、UGC（User-Generated Content）和热门推荐（Top Seller/Trending）的协作。</p>
<ul>
<li><strong>相似度矩阵（Similarity Matrix）：</strong><br>大家提的各种算法里面，几乎都是基于相似度的吧 — 无论是CF还是Content based产生的相似度，前者需要用户的行为数据，后者需要歌曲的元数据（metadata），比如旋律、Tag等等。<br>需要避免过多推荐单一歌手，避免过多热门。<br>找到冷门优秀的歌曲。<br>类似与tf-idf，在歌曲对个人的重要性在总榜里的热度乘反比，在个人的喜好程度乘正比。</li>
</ul>
<hr>
<h1 id="userCF算法和itemCF算法的层面"><a href="#userCF算法和itemCF算法的层面" class="headerlink" title="userCF算法和itemCF算法的层面"></a>userCF算法和itemCF算法的层面</h1><blockquote>
<p>链接：<a href="https://link.jianshu.com?t=https://www.zhihu.com/question/26743347/answer/65777210" target="_blank" rel="noopener">作者：郑昊</a><br>来源：知乎著作权归作者所有，转载请联系作者获得授权。<br>在本文中我们将提到两种方法来实现这个目的，基于用户的协作型过滤和基于物品的协作型过滤。</p>
</blockquote>
<h2 id="基于用户的协作型过滤"><a href="#基于用户的协作型过滤" class="headerlink" title="基于用户的协作型过滤"></a>基于用户的协作型过滤</h2><p> 音乐用户甲-&gt;偏好相近用户-&gt;相关歌曲-&gt;推荐列表</p>
<p>流程至少包括以下四个步骤：<br>建立评价规则<br>搜集用户偏好<br>寻找相近的用户<br>推荐歌曲</p>
<blockquote>
<p><strong>1.建立评价规则</strong><br>下图是我随意做的一个评价规则。评价规则应该根据明确的用户行为来建立。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191119203521.png" alt></p>
<p>评价规则-随意做的</p>
<p><strong>2.搜集用户偏好</strong><br>根据评价规则，我们可以得到每个用户和该用户相关的每首歌的一个得分。 下图也是我随意造的数据。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191119203536.png" alt></p>
<p>用户偏好</p>
<p><strong>3.寻找相近的用户</strong><br>常用的计算相似度评价值的体系有两种：欧几里得距离和皮尔逊相关度。</p>
<p><strong>4.推荐歌曲</strong><br>接下来系统要做的就是，为用户郑昊提供歌曲推荐。我们当然可以查找与郑昊品味最相近的人，从他所喜欢的歌曲中找出一首郑昊可能还未接触过的歌曲。不过，这样的做法未免太随意了。<br>目前最通用的做法是，通过一个经过加权的评价值来为歌曲打分，评分结果即排名结果。为此，我们需要取得所有其他用户的分数，借此得到相关系数后，再乘以他们与相关歌曲的分数，求和之后再除以对应的相关系数总计，便能获得一个我们需要的评价值。在下表中我们给出了具体的做法。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191119203705.png" alt></p>
<p>「相关系数」一列来自于皮尔逊相关度评价。「歌名」对应各用户的得分来自评价规则处理后的结果。将前两者一一对应相乘，便是「歌N*相关系数」的值。如此一来，相比于与我们不相近的人，那些与我们相近的人将会对整体评价值拥有更多的贡献。总计一行给出了所有加权评价值的总和。</p>
<p>我们可以用总计值来计算歌曲排名，但是我们还需要考虑到，这样人数会对一首歌的得分产生正相关影响。为了避免这一问题，我们需要将总计除以相关系数总计。相关系数总计等于所有对这首歌曲有影响的用户的相关系数之和。表中最后一行就是我们所需要的结果。</p>
</blockquote>
<h2 id="基于物品的协作型过滤"><a href="#基于物品的协作型过滤" class="headerlink" title="基于物品的协作型过滤"></a>基于物品的协作型过滤</h2><p>1.歌曲A-&gt;相关用户-&gt;相关歌曲-&gt;推荐列表；<br>2.网易云音乐用户甲-&gt;偏好歌曲-&gt;推荐列表。</p>
<p>1是主要计算过程，2是推荐过程。</p>
<hr>
<h1 id="从产品角度思考数据"><a href="#从产品角度思考数据" class="headerlink" title="从产品角度思考数据"></a>从产品角度思考数据</h1><p><a href="http://www.chanpin100.com/article/100147" target="_blank" rel="noopener">链接</a><br>行业分析、市场分析、用户群的划分和分析之外，还考虑未来的发展方向</p>
<hr>
<h1 id="实习经历描述"><a href="#实习经历描述" class="headerlink" title="实习经历描述"></a>实习经历描述</h1><p>腾讯 微信事业群 数据分析实习生 用户拉起方向 2019.04-2019.08 l 运用聚类算法对用户来源、拉起特征、活跃企业等属性完成了用户聚类，梳理出用户画像，找到用户拉起的增长点;通过通过漏斗分析，寻找出注册页瓶颈，找到改进措施。</p>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191119221035.png" alt></p>
<p>产品岗，做的杂：捞数据，做报表，策划，数据分析。<br>分析：互通现状。<br>任务：分析现状，梳理出下一步的方向。<br>What：现状是什么？用户增长放缓，发消息加深。<br>思路：用户群的划分→得到比例和画像→比例与市场比较，寻找空间；画像对比我们的目标企业查看问题</p>
<ul>
<li><p>用户群怎么划分？<br>1 活跃 62%<br>2 基于产品性质，协同12%，服务50%<br>3 服务类的 基于使用情况，公司规模 大企业10% 小企业40%</p>
</li>
<li><p>画像，使用情况：<br>分词去寻找岗位，树状结构去找级别，流失时间点，是否体验了</p>
</li>
</ul>
<p>解决：<br>when：使用互通次日<br>who：IT，零售，物流<br>why：需要客户联系，管理消费者资源<br>how：<br>1 大企业，推联系方式给BD联系主动沟通，拉起使用<br>2 中小企业，非管理员体验，push引导管理员开通客户联系；  管理员体验，企业号推送行业案例，激发兴趣<br>3 未进入的企业，寻找“优质用户”包，在朋友圈广告定向推送互通行业案例<br>4 增加通道：微信个人资料增加企业微信icon，增加曝光</p>
<hr>
]]></content>
      <tags>
        <tag>实习</tag>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title>What is GAN basicly?</title>
    <url>/2019/What-is-GAN-basicly/</url>
    <content><![CDATA[<p><strong>MSBD5012 Term Paper</strong></p><p>Due Date: 7 December 2019</p><p>Via Canvas</p><p>As announced by the university administration, there won’t be any proctored examinations this semester and alternative assessment arrangements are to be made by the course instructors. For CSIT6000G/MSBD5012, you are asked to write a term paper in lieu of the final exam.</p><a id="more"></a>



<p>You need to choose one topic from the following list, and explain it informally:</p>
<p>· Adversarial attack</p>
<p>· Variational autoencoders</p>
<p>· Generative adversarial networks</p>
<p>· Deep reinforcement learning</p>
<p>The targeted readers of the paper are computer science students who are about to take the course. In other words,  <strong>you are asked to explain the chosen topic to the past you</strong> at 1 September 2019.  Obviously, there isn’t enough space for you to include all the details. However, you need to cover the<br>key concepts and key ideas.<br> You can follow the outlines of the relevant lectures, and might need to include contents before those lectures as background.</p>
<p>You can include diagrams and mathematical formulae. However, avoid mathematical formulae as much as possible because  <strong>the purpose is to give informal explanations, not formal proofs.</strong></p>
<p>The term paper should be no more than 4 pages in length, and the font size of the main text should be 12pt. You are encouraged to use latex  <a href="https://github.com/ICLR/Master-Template/blob/master/archive/iclr2020.zip" target="_blank" rel="noopener">latex template</a> of <a href="https://iclr.cc/Conferences/2020/CallForPapers" target="_blank" rel="noopener">ICLR 2020</a>. Generate a pdf file for submission via Canvas and name your file  “ [Last Name]_ [First Name]_[Student ID].pdf”. Discussions among students are encouraged. However, you need to write up your paper independently.  A plagiarism checker will be run on all submitted reports.</p>
<p>The term paper will be graded using the following scheme:</p>
<p>· Overall understanding of the topic:  50%</p>
<p>· Clarity of explanations:  30%</p>
<p>· Effort (how polished the report is):  20%</p>
<p>The term paper is  <strong>due by 23:59 on 7 December</strong>, the scheduled final exam date.  <strong>No late submissions will be accepted</strong>.</p>
<hr>
<h1 id="Generative-Adversarial-Networks-is-so-Easy"><a href="#Generative-Adversarial-Networks-is-so-Easy" class="headerlink" title="Generative Adversarial Networks is so Easy"></a>Generative Adversarial Networks is so Easy</h1><p>In machine learning course, we learn how to teach our program to learn the information among the data. There are a lot’s kinds of work can be down, just like classification, regression and generate new data as same as nature. Generating new data is pretty interesting job. Thinking about what if you can teach your program to learn the plot by Picasso’s. The plots by Picasso’s  is very famous and expensive. The most painting by Picasso is _Les femmes d’Alger_  which worth  $179 million.  If we can teach our program to plot a image similar with his style easily. That’s will be a amazing job. But now we can made it by the algorithm GANs.</p>
<h2 id="Basic-idea"><a href="#Basic-idea" class="headerlink" title="Basic idea"></a>Basic idea</h2><p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191130001221.png" alt></p>
<p>GANs are the algorithms represented for Generative Adversarial Networks.  It is the process of two complex algorithms (neural networks) competing against each other. One algorithms is called <strong>Generator,</strong> the other algorithm is called <strong>Discriminator</strong>.</p>
<p>The generative model and the discriminant model play a game with each other and learn to produce quite good output. Taking pictures as an example, the main task of the generator is to learn the real picture set, so that the pictures generated by yourself are closer to the real pictures, and the “disguise” discriminator. The main task of the discriminator is to find out the picture generated by the generator, distinguish it from the real picture, and perform true and false discrimination. Throughout the iteration process, the generator continuously strives to make the generated image more and more real, and the discriminator continuously strives to identify the authenticity of the picture. This is similar to the game between the generator and the discriminator. After repeated iterations, the two finally reached a balance: the picture generated by the generator is very close to the real picture, and it is difficult for the discriminator to distinguish the difference between the real and fake pictures. Its performance is that for true and false pictures, the probability output of the discriminator is close to 0.5.</p>
<p>Let’s still assume an I wants to replicate the style of Picasso. After I have watch all the detail of Picasso’s paintings, I think I have learned a lot. So I find a a collector to help me improve my level. The collector has rich experience and sharp eyes, and the paintings on the market that imitate Picasso cannot escape his eyes. The collector told me a word: when will your painting deceive me, you will be successful.</p>
<p>Then I give hime this one:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191130110753.png" alt><br>The collector glanced lightly and was very angry. “0 points! This is also called painting? Too much difference!” After listening to the collector’s words, I began to reflect on myself and did not hesitate to draw, even it is a black image. So I drew another picture:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191130110852.png" alt></p>
<p>The collector saw : 1 point ! Repaint! As soon as I thought it was still impossible, the painting was too bad, so I went back to study Picasso’s painting style, and continued to improve and re-create, until one day I showed the new painting to the collector:<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191130001221.png" alt><br>This time, the collector was wearing glasses and carefully analyzing. After a long time, the collector patted my shoulder and said that the painting was very good. Haha, I was so happy to be praised and affirmed by the collector.</p>
<p>This example is actually a GAN training process. I am a generator, the purpose is to output a picture that can fool collectors, making it difficult for collectors to distinguish between true and false! The collector is the discriminator, the purpose is to identify my painting and judge it to be false! The whole process is a game of “generation-confrontation”. In the end, I (the generator) outputs a picture of “truths and false truths”, and even collectors (the discriminator) can hardly distinguish.</p>
<h2 id="What’s-GAN-model"><a href="#What’s-GAN-model" class="headerlink" title="What’s GAN model?"></a>What’s GAN model?</h2><p>After we talk about the basic ideal, then let’s see what is the Generative Adversarial Networks(GANs) mathematically.  Generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture.</p>
<p>The GAN architecture was first described in the 2014 paper by  Ian Goodfellow, et al. titled “Generative Adversarial Networks.” After this paper appeared, there are plenty related paper followed. A standardized approach called Deep Convolutional Generative Adversarial Networks, or DCGAN, that led to more stable models was later formalized by  Alec Radford, et al. in the 2015 paper titled Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.</p>
<p>The GAN model architecture involves two sub-models: a  _generator model_  for generating new examples and a  _discriminator model_  for classifying whether generated examples are real, from the domain, or fake, generated by the generator model.</p>
<ul>
<li><strong>Generator</strong>. Model that is used to generate new plausible examples from the problem domain.</li>
<li><strong>Discriminator</strong>. Model that is used to classify examples as real (_from the domain_) or fake (_generated_).</li>
</ul>
<blockquote>
<p>Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator.</p>
</blockquote>
<h3 id="Generator-Model"><a href="#Generator-Model" class="headerlink" title="Generator Model"></a>Generator Model</h3><p>The generator model takes a fixed-length random vector as input and generates a sample in the domain. The vector is drawn from randomly from a Gaussian distribution, and the vector is used to seed the generative process. After training, points in this multidimensional vector space will correspond to points in the problem domain, forming a compressed representation of the data distribution.</p>
<p>This vector space is referred to as a latent space, or a vector space comprised of  <a href="https://en.wikipedia.org/wiki/Latent_variable" target="_blank" rel="noopener">latent variables</a>. Latent variables, or hidden variables, are those variables that are important for a domain but are not directly observable.</p>
<p>We often refer to latent variables, or a latent space, as a projection or compression of a data distribution. That is, a latent space provides a compression or high-level concepts of the observed raw data such as the input data distribution. In the case of GANs, the generator model applies meaning to points in a chosen latent space, such that new points drawn from the latent space can be provided to the generator model as input and used to generate new and different output examples.</p>
<p>After training, the generator model is kept and used to generate new samples.</p>
<p><img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/Example-of-the-GAN-Generator-Model.png" alt="Example of the GAN Generator Model"></p>
<p>Example of the GAN Generator Model</p>
<h3 id="The-Discriminator-Model"><a href="#The-Discriminator-Model" class="headerlink" title="The Discriminator Model"></a>The Discriminator Model</h3><p>The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake (generated).</p>
<p>The real example comes from the training dataset. The generated examples are output by the generator model.</p>
<p>The discriminator is a normal (and well understood) classification model.</p>
<p>After the training process, the discriminator model is discarded as we are interested in the generator.</p>
<p>Sometimes, the generator can be repurposed as it has learned to effectively extract features from examples in the problem domain. Some or all of the feature extraction layers can be used in transfer learning applications using the same or similar input data.</p>
<p><img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/Example-of-the-GAN-Discriminator-Model.png" alt="Example of the GAN Discriminator Model"></p>
<h1 id="Combine-two-parts"><a href="#Combine-two-parts" class="headerlink" title="Combine two parts"></a>Combine two parts</h1><p>Generative modeling is an unsupervised learning problem, as we discussed in the previous section, although a clever property of the GAN architecture is that the training of the generative model is framed as a supervised learning problem. The two models, the generator and discriminator, are trained together. The generator generates a batch of samples, and these, along with real examples from the domain, are provided to the discriminator and classified as real or fake. The discriminator is then updated to get better at discriminating real and fake samples in the next round, and importantly, the generator is updated based on how well, or not, the generated samples fooled the discriminator.</p>
<p>In this way, the two models are competing against each other, they are adversarial in the game theory sense, and are playing a  zero-sum game. In this case, zero-sum means that when the discriminator successfully identifies real and fake samples, it is rewarded or no change is needed to the model parameters, whereas the generator is penalized with large updates to model parameters. Alternately, when the generator fools the discriminator, it is rewarded, or no change is needed to the model parameters, but the discriminator is penalized and its model parameters are updated.</p>
<p>At a limit, the generator generates perfect replicas from the input domain every time, and the discriminator cannot tell the difference and predicts “unsure” (e.g. 50% for real and fake) in every case. This is just an example of an idealized case; we do not need to get to this point to arrive at a useful generator model.</p>
<p><img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/Example-of-the-Generative-Adversarial-Network-Model-Architecture.png" alt="Example of the Generative Adversarial Network Model Architecture"></p>
<p>Example of the Generative Adversarial Network Model Architecture</p>
]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习GDBT,XGBOOST,RF</title>
    <url>/2019/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0GDBT-XGBOOST-RF/</url>
    <content><![CDATA[<p>集成学习（Ensemble Learning），集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。</p><p>集成学习致分为两大类：</p><ul>
<li>Boosting:即个体学习器之间存在强依赖关系、必须串行生成的序列化方法，Adaboost, GDBT, Xgboost.</li>
<li>Bagging以及个体学习器间不存在强依赖关系、可同时生成的并行化方法，“随机森林”（Random Forest）。</li>
</ul><a id="more"></a>


<h1 id="1-Bagging"><a href="#1-Bagging" class="headerlink" title="1. Bagging"></a>1. Bagging</h1><p>Bagging：简单放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，无依赖关系。</p>
<h2 id="1-1-随机森林"><a href="#1-1-随机森林" class="headerlink" title="1.1 随机森林"></a>1.1 随机森林</h2><p>Random Forest（随机森林）：Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分：<br>1、随机选择 样本（放回抽样）；<br>2、随机选择 特征；<br>3、构建分类器；如：ID3、C4.5、CART、SVM、Logistic regression等<br>4、投票（平均）</p>
<p>随机性偏差会有微增（相比于单棵不随机树），‘平均’会使得方差减小更多</p>
<ul>
<li>随机森林的优点<br>1、速度快，精度不会很差<br>2、能够处理高维数据，不用特征选择，训练完后，可给出特征重要性；<br>3、可并行化  </li>
<li>随机森林的缺点：在噪声较大的分类或者回归问题上回过拟合。</li>
</ul>
<h1 id="2-Boosting"><a href="#2-Boosting" class="headerlink" title="2. Boosting"></a>2. Boosting</h1><h2 id="2-1-基于调整权重-Adaboost"><a href="#2-1-基于调整权重-Adaboost" class="headerlink" title="2.1 基于调整权重 Adaboost"></a>2.1 基于调整权重 Adaboost</h2><p>每生成一棵树之后，计算两个权重</p>
<blockquote>
<p>1 计算这个树的误差率，误差率越高，权重越低<br>2 计算每个样本的错分率，错分的样本，权重越高，之后更容易分对</p>
</blockquote>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116091802.png" alt><br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116091915.png" alt></p>
<h2 id="2-2-基于残差：GB-GBTD-Xgboost"><a href="#2-2-基于残差：GB-GBTD-Xgboost" class="headerlink" title="2.2 基于残差：GB(GBTD,Xgboost)"></a>2.2 基于残差：GB(GBTD,Xgboost)</h2><h3 id="2-2-1-GBTD"><a href="#2-2-1-GBTD" class="headerlink" title="2.2.1 GBTD"></a>2.2.1 GBTD</h3><p>GBDT只能由回归树组成.</p>
<ul>
<li>基本思想：<br>在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法。</li>
</ul>
<p><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116110339.png" alt></p>
<ul>
<li>如果是分类树，损失函数是指数损失函数：<br>𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))</li>
<li>如果是回归树，损失函数是均方损失（CART）：<br>𝐿(𝑦,𝑓(𝑥))=(𝑦−𝑓(𝑥))^2</li>
<li>如何防止过拟合？<blockquote>
<ol>
<li>步长v(0-1)，权重衰减 𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+𝑣 ℎ𝑘(𝑥)，降低新来的分类器的影响力</li>
<li>子采样比例</li>
<li>用弱学习器</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="2-2-2-xgboost"><a href="#2-2-2-xgboost" class="headerlink" title="2.2.2 xgboost"></a>2.2.2 xgboost</h3><p>当已经生成了一棵树的时候，如何去选择新子树：</p>
<ul>
<li><p>1 目标函数：</p>
<script type="math/tex; mode=display">Obj=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k} \Omega\left(f_{k}\right), f_{k} \in \mathcal{F}</script></li>
<li><p>1.1 第t轮的时候：</p>
<script type="math/tex; mode=display">\begin{aligned} O b j^{(t)} 
&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t)}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) 
\\ & \equiv \sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+c\end{aligned}</script></li>
<li>这时候需要寻找f_t来让目标函数最小</li>
</ul>
<blockquote>
<p>a. 式子左边，对目标函数在$f_t(x)$上泰勒展开，去二阶，求得近似解：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116120113.png" alt></p>
<p>b. 式子右边，定义复杂度 $\Omega\left(f_{t}\right)$<br>每颗树，都是由枝干(分类节点)和叶子(树的末端)组成的。<br>定义复杂度为：叶子个个数T, 加上每个叶子的值w平方和（各有系数）。<br>树越复杂，T ↑，w平方和 ↑，复杂度 ↑，惩罚 ↑。<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116121238.png" alt></p>
</blockquote>
<ul>
<li>更新目标函数：<script type="math/tex; mode=display">
\begin{aligned} O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) +c\\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\left(\gamma T+\lambda \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2}\right)+c
\\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T +c \end{aligned}</script></li>
</ul>
<blockquote>
<ul>
<li>其中$f_{t}(x)=w_{q(x)}, w \in \mathbf{R}^{T}, q: \mathbf{R}^{d} \rightarrow\{1,2, \cdots, T\}$，表示x→叶节点→对应的值w。目的是统一用$w_{j}$表示树$f_t$。</li>
<li>第三行的理解，对于示性函数$I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$，$I_j$表示一个集合在j的叶子节点中。用示性函数求和代替1-n的求和，然后交换求和顺序。</li>
</ul>
</blockquote>
<ul>
<li>这里发现目标函数是关于$w_{j}$的二次函数，二次函数在对称轴上取极值。<br>简化表达：$G_{j}=\sum_{i \in I_{j}} g_{i} \quad H_{j}=\sum_{i \in I_{j}} h_{i}$得到：<script type="math/tex; mode=display">
\begin{aligned} O b j^{(t)} =\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}</script>二次函数求极值得到：<script type="math/tex; mode=display">
\begin{array}{c}{w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}} \\ {} \\ {O b j=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T}\end{array}</script>w是每一个叶节点值，Obj是这个树的分数，分数越低越好。<br>这样，就可以直接计算出树的分数，可以对于候选进行评比。</li>
</ul>
<hr>
<p>如何生成候选树？</p>
<ul>
<li>Enumerate 枚举可能的结构</li>
<li>通过刚才的式子计算最优分数</li>
<li>但是问题是有无限的可能性。</li>
</ul>
<hr>
<ul>
<li>所以通过贪婪学习：（不详细讲）<br>每一次尝试对已有的叶子结点加入一个分割，选择具有最佳增益的分割对结点进行分裂。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算：<br><img src="https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116142330.png" alt></li>
</ul>
<blockquote>
<p>也就是通过信息增益去寻找最优分割点。<br>这里有个好处就是如果惩罚大于增益，gain就会为负数，自动停止。</p>
</blockquote>
<h1 id="3-模型对比"><a href="#3-模型对比" class="headerlink" title="3 模型对比"></a>3 模型对比</h1><h2 id="3-1-随机森林vsGDBT"><a href="#3-1-随机森林vsGDBT" class="headerlink" title="3.1 随机森林vsGDBT"></a>3.1 随机森林vsGDBT</h2><ul>
<li>决策树类型：组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成；  </li>
<li>结果预测：对于最终的输出结果而言，随机森林采用多数投票、简单平均等；而GBDT则是将所有结果累加起来，或者加权累加起来；  </li>
<li>并行/串行：组成随机森林的树可以并行生成；而GBDT只能是串行生成；  </li>
<li>异常值：随机森林对异常值不敏感；GBDT对异常值非常敏感；</li>
<li>方差/偏差：随机森林减少方差；GBDT是通过减少偏差。</li>
</ul>
<h2 id="3-2-GDBT-vs-XGboost"><a href="#3-2-GDBT-vs-XGboost" class="headerlink" title="3.2 GDBT vs XGboost"></a>3.2 GDBT vs XGboost</h2><ul>
<li>GDBT只支持CATR树，xgboost还支持线性分类器</li>
<li>GDBT只用了一阶，xgboost泰勒展开，用了二阶</li>
<li>xgboost有正则项，而且会自动停止生成(依赖参数gamma)。</li>
<li>xgboost可以列抽样，借鉴了随机森林的做法</li>
<li>XGBOOST可以自动学习出缺失值的分裂方向</li>
<li>XGBOOST实现了并行化：每个特征并行计算，每个特征划分也并行计算</li>
</ul>
<p>后期实现中，xgboost还有优化，所以很快：</p>
<ul>
<li>在寻找分割点，枚举贪心法效率低，xgboost实现近似的算法。大致的思想是根据百分位法列举几个成为分割点的候选者，然后再进一步计算。</li>
<li>xgboost考虑了训练数据为稀疏值的情况（我也不懂T^T）</li>
<li>特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。</li>
<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</li>
</ul>
<p>总结，一个简单的idel不断优化，借鉴别的想法，优化到了极致，导致xgboost能这么强。</p>
<p>参考资料：xgboost原文<br><a href="https://blog.csdn.net/weixin_42158523/article/details/81737370" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42158523/article/details/81737370</a><br><a href="https://www.cnblogs.com/aixiao07/p/11375168.html" target="_blank" rel="noopener">https://www.cnblogs.com/aixiao07/p/11375168.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>港校msc 互联网找工作时间轴</title>
    <url>/2019/Blog2-%E6%B8%AF%E6%A0%A1msc-%E4%BA%92%E8%81%94%E7%BD%91%E6%89%BE%E5%B7%A5%E4%BD%9C%E6%97%B6%E9%97%B4%E8%BD%B4/</url>
    <content><![CDATA[<h1 id="港校msc-互联网找工作时间轴"><a href="#港校msc-互联网找工作时间轴" class="headerlink" title="港校msc 互联网找工作时间轴"></a>港校msc 互联网找工作时间轴</h1><p><strong>入学前的（大四） 暑期实习</strong> 建议参加<br>3-5月 实习岗位网申开启<br>5-7月 进行网申、面试，发放Offer<br>7-9月 进入公司实习</p><p><strong>入学前的 秋季校招</strong> 建议参加<br>7-8月 互联网大厂的提前批、正式校招开放；<br>9月意向书，10月底谈薪</p><a id="more"></a>

<hr>
<h2 id="入学"><a href="#入学" class="headerlink" title="入学"></a>入学</h2><p><strong>入学后的</strong> 春季校招 招人较少<br>2-4月 进行网申<br>3-5月 开始笔试、面试<br>5-6月 发Offer入职</p>
<p><strong>暑假实习</strong> 必参加<br>3-5月 各大行业实习岗位网申开启<br>5-7月 进行网申、面试，发放Offer<br>7-9月 进入公司实习</p>
<p><strong>秋季校招</strong> 即使拿到实习return也建议参加，有利于argue涨价<br>7-8月 互联网大厂的提前批、正式校招开放；<br>9月意向书，10月底谈薪</p>
<p>11月毕业-&gt;入职</p>
<hr>
<h2 id="成为社畜"><a href="#成为社畜" class="headerlink" title="成为社畜"></a>成为社畜</h2><p>我的个人准备笔试面试时间轴：<br><!-- Table --><br>| 月份| 校内|找工作准备|<br>|—|—|—|<br>| 11月| 各种ddl | 算法基础+数据结构<br>|12月|考试+寒假|剑指offer<br>|1月|寒假| 剑指offer+NLP+整理机器学习+特征工程<br>|2月|开学不忙|leetcode mid+NLP实战+整理深度学习<br>|3月|开始准备期中| leetcode hard+笔试概率题、智商题</p>
]]></content>
      <tags>
        <tag>timeline</tag>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title>Build by Hexo</title>
    <url>/2019/Blog1-Build%20by%20Hexo/</url>
    <content><![CDATA[<p>本次搭建blog，完全学习于：<a href="https://www.bilibili.com/video/av44544186?from=search&amp;seid=6748505739751413370" target="_blank" rel="noopener">b站up主codesheep视频</a></p><p>下面进入流程：</p><ol>
<li>sudo su 进入管理员</li>
<li>安装Node.js，搜索，下载，安装<br>安装之后会有两个工具： node和npm<br>也可以用国内的cnpm</li>
<li>通过npm安装hexo 博客静态框架<blockquote>
<p>npm install -g hexo-cli </p>
</blockquote>
</li>
<li>建立一个专有的文件夹，方便管理</li>
<li>文件夹下运行hexo<blockquote>
<p>sudo hexo init</p>
</blockquote>
</li>
</ol><a id="more"></a>


<p>初始化文件，主要的有_config.yml 配置文件，source内容的文件夹，themes主题文件夹。</p>
<blockquote>
<p>hexo s #start 开始<br>hexo n “文章名” #生成文章<br>hexo clean #清理之前的</p>
<ol>
<li>部署到github</li>
</ol>
</blockquote>
<p>生成 xxx.github.io（ xxx必须为github的用户名）的项目</p>
<blockquote>
<p>npm install —save hexo-deployer-git</p>
</blockquote>
<p>下载插件，连接到git</p>
<p>设置_config.yml最下面</p>
<blockquote>
<p>type: git<br>repo: <a href="https://github.com/xxxx/xxxx.github.io" target="_blank" rel="noopener">https://github.com/xxxx/xxxx.github.io</a></p>
</blockquote>
<p>推送到远端</p>
<blockquote>
<p>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
</blockquote>
<p>访问 xxxx.github.io，就可以看到自己的博客啦。</p>
<hr>
<p><a href="https://hexo.io/zh-cn/docs/commands.html" target="_blank" rel="noopener">hexo官网</a></p>
]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Blog</title>
    <url>/2019/Hello-Blog/</url>
    <content><![CDATA[<h1 id="Hello-Blog"><a href="#Hello-Blog" class="headerlink" title="Hello Blog."></a>Hello Blog.</h1><p>之前经常会发现大佬有自己的技术博客，之前也尝试着去做一个，但是由于自己的技术水平有限，也没有决定好走技术路线，所以就一直没有开始写技术博客。</p><p>最近比较了算法和产品的待遇，真的差别好大。暂且不说之后的发展会怎样，程序员/技术岗本身是智力和努力的比拼，是硬功夫。有更清晰的发展方向。再加上种种原因，我决定做技术了。</p><a id="more"></a>

<ul>
<li>所以为什么要写博客呢？<br>（鉴于自己的理解和b站up主<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2Fav56930990%2F%3Fspm_id_from%3D333.788.videocard.2" target="_blank" rel="noopener">codesheep</a> 的介绍，总结如下） <ol>
<li>博客=输出=实践<br>写博客也是一个技术输出的过程，而当你想输出的时候，你就已经需要整合自己的学习成果，不断的理解技术细节，比如说：神经网络的梯度传到过程，虽然接触很多遍，不动手就很难掌握。</li>
<li>博客=输出=表达<br>理科出身的我经常给自己找借口，原理我懂就行，表达不清楚就慢慢表达，但实际上，表达不清楚会导致别人不愿意和你交流，从而丧失很多机会。<br>有了输出的过程，就需要去磨炼自己表达的精炼程度，以及练习如何让别人明白，就更容易理解。</li>
<li>博客=简历=社交<br>有了个人主页，别人就知道你的技术水平，你会什么，你学过什么，一应俱全。是找工作，或者是技术交流的好平台。</li>
</ol>
</li>
<li>担心自己的博客没有技术含量？<br>你觉得没有技术含量的可能会对别人有用，只要可复现，都是有价值的。</li>
<li>现在写的会晚吗？<br>我知道很多cs的同学大一就开始搭建自己的知识体系，构建专栏，值得敬佩和羡慕。不过人生漫长，任何时候开始干一件事都不晚。</li>
<li>写什么内容？<ol>
<li>学习笔记、心得</li>
<li>生活感悟</li>
<li>工程排坑</li>
</ol>
</li>
</ul>
<p>为了混口饭吃，写博客的flag没有任何理由推倒了吧。<br>希望最少保持一个月一篇的频率，正常频率一周一篇。</p>
]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
