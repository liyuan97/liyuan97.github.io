title: CS224n-3 回顾神经网络，文本分类介绍
author: Yuan
mathjax: true
tags:
  - NLP
categories:
  - CS224n
date: 2020-02-15 23:37:47
---
本节课主要是：
分类器回顾，神经网络回顾，NER  Named Entity Recognition，窗口词分类，矩阵计算回顾。总体是回顾旧知识，以及nlp任务的举例，快速过一下。

# 分类
分类问题很常见了，对于NLP来说：
- x ： 单词，句子，文章
- y ：情感，是否NER，决策判别，是否人话

## 回顾softmax：
$$
p(y | x)=\frac{\exp \left(W_{y} \cdot x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}
$$

分子：输入向量和矩阵的一行相乘
分母：所有相乘，归一化

训练可以直接最大化概率，或者最小化负log概率。
$$p(y_j = 1|x) = \frac{\exp(W_{j\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)}$$

但是，巧合的是，最小化负log概率对于softmax等同于cross entropy。

>  回顾cross entropy：
>交叉熵来源于信息论，对于对于c情况真实概率p，模型计算的概率q，那么交叉熵计算为：$H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)$

对于softmax的交叉熵来说：
$$
\begin{aligned} H(\hat{y}, y) &=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right) \\ &=-\sum_{j=1}^{C} y_{j} \log \left(p\left(y_{j}=1 | x\right)\right) \\ &=-\sum_{j=1}^{C} y_{j} \log \left(\frac{\exp \left(W_{j}, x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}\right) \\ &=-y_{i} \log \left(\hat{y}_{i}\right) \end{aligned}
$$
最后一个等号的求和去掉是因为，只有当$y_i$预测对了，才会累加，否则都是0.

所以，交叉熵和负对数概率是等价的。

对于有N个样本的数据集，整体的损失就是：
$$
-\sum_{i=1}^{N} \log \left(\frac{\exp \left(W_{k(i)} \cdot x^{(i)}\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x^{(i)}\right)}\right)
$$

> 括号里就是对于每一个样本x的预测值概率大小，所有都猜对的话，概率是1，整体损失就是0；当没猜对，概率在0-1，-log(0. xxx)就是很小，损失就多。

一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222013518.png)

# 神经元的本质
如果损失函数用sigmoid，单个神经元就是逻辑回归：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222014539.png)

而神经网络，就是多个逻辑回归同时计算，如图：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222014651.png)
上图就是四个逻辑回归(经过4次激活函数)

# Named Entity Recognition (NER)

实体识别就是把句子里的实物名字找出来，并且进行分类，比如说组织单位，个人名字，电器名字，软件名字，地点之类的。
用途：
- 定位前文的主语
- 问答系统需要实体
- 信息一般都与实体相关
-  slot-filling 填空问题

由于自然语言中，大部分实体模糊不清，代词也会干扰，所以不简单。

> 实体识别就是把句子里是实物找出来

# 窗口单词分类Binary word window classification


单独对于一个单词分类很难，所以简单思路就是取前后的四五个单词窗口，取平均（或者把向量串起来），然后再进行分类。

但是有一个问题，平均会丢失单词在窗口里的位置。

![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222214054.png)


对于位置问题，可以通过打分来解决（回归）。让位置在中间的样本y高，不在中间的样本（打乱顺序）对应的yscore低。如图。

![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222214634.png)
# 间隔最大化目标函数

这里上课老师跳过了，没看懂先挖个坑
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222215915.png)

# 反向传播
求导，链式法则，老生常谈了。

关注一下矩阵的求导：

![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222220647.png)

对于一个m输出，n输入的函数，求导就是对于m行，每行求对应位置输入x_j的导数。

n个输入n个输出：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222221411.png)
所以，可以得到一个只有对角线元素上有导数，其他位置为0的矩阵。

所以，最后可以得到s（最后的分数）对b(线性计算加的那个偏置)的导数：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222222311.png)

上图中，如果同时对w和b求导可以发现：
![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222222558.png)

观察出，b的导数就是信号的损失

上图第一行，z对w的求导就是x。所以可以得到s对于w的导数，其实就是delt损失和x相乘。

![](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20200222223106.png)

所以导数也是n*m的。

> 这里没讲完就下课了...，主要是想展示计算过程吧。